<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Web Mining - Ibara99 Doc</title>
        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="..">Ibara99 Doc</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="..">Home</a>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">My Project <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li class="active">
    <a href="./">Web Mining</a>
</li>
                                </ul>
                            </li>
                            <li >
                                <a href="../about/">About Me</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="..">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../about/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#simple-web-crawler">Simple Web Crawler</a></li>
        <li class="main "><a href="#pengantar">Pengantar</a></li>
        <li class="main "><a href="#environment">Environment</a></li>
        <li class="main "><a href="#teori-dan-penjelasan-program">Teori dan Penjelasan Program</a></li>
            <li><a href="#crawling">Crawling</a></li>
            <li><a href="#text-extraction">Text-Extraction</a></li>
            <li><a href="#preprocessing">Preprocessing</a></li>
            <li><a href="#procesing">Procesing</a></li>
        <li class="main "><a href="#analisis">Analisis</a></li>
            <li><a href="#tanpa-dilakukan-preprocessing">Tanpa dilakukan preprocessing</a></li>
            <li><a href="#berdasarkan-jumlah-cluster">Berdasarkan Jumlah Cluster</a></li>
            <li><a href="#berdasarkan-nilai-threshold">Berdasarkan nilai Threshold</a></li>
            <li><a href="#berdasarkan-n-gram">Berdasarkan N-Gram</a></li>
            <li><a href="#k-means_1">K-Means</a></li>
        <li class="main "><a href="#kesimpulan">Kesimpulan</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="simple-web-crawler">Simple Web Crawler</h1>
<p>â€‹   <em>Program ini digunakan untuk mengekstrak data dari sebuah website. Untuk</em> source code <em>lengkap, sila klik <a href="https://github.com/Ibara99/Simple-Web-Crawling">di sini</a>.</em></p>
<h1 id="pengantar">Pengantar</h1>
<blockquote>
<p>Ucapan terima kasih kepada Bapak <a href="https://forlap.ristekdikti.go.id/dosen/detail/RTA5QTg4RjctMjBEQy00QThELUI4REYtREQ5ODAzMzU0MjUz">Mulaab, S.Si., M.Kom.</a> selaku Dosen Pembimbing kami</p>
<p>Nama          : Ibnu Asro Putra</p>
<p>NRP           : 160411100023</p>
<p>Mata kuliah       : Penambangan dan Pencarian Web - 2019</p>
<p>Jurusan           : Teknik Informatika</p>
<p>Perguruan Tinggi  : Universitas Trunojoyo Madura</p>
</blockquote>
<h1 id="environment">Environment</h1>
<p><em>Program ini dijalankan menggunakan:</em></p>
<ul>
<li>Bahasa Python, dengan library:</li>
<li>BeautifulSoup4 (install menggunakan pip)</li>
<li>requests (install menggunakan pip)</li>
<li>SQLite3 (library bawaan python)</li>
<li>csv  (library bawaan python)</li>
<li>numpy (install menggunakan pip)</li>
<li>scipy (install menggunakan pip)</li>
<li>scikit-learn (install menggunakan pip, perlu untuk install numpy dan scipy terlebih dulu)</li>
<li>Scikit-fuzzy (install menggunakan pip, perlu untuk install numpy dan scipy terlebih dulu)</li>
<li>Website target : <a href="https://garuda.ristekdikti.go.id">Jurnal Online</a>)</li>
</ul>
<blockquote>
<p>Jika anda memutuskan untuk meng-update data, maka Program <strong>hanya</strong> bisa dijalankan menggunakan Internet, atau program akan error</p>
<p>Setiap kali program dijalankan, akan muncul file baru bernama <code>test.db</code>, dan beberapa file <code>csv</code>. File tersebut merupakan file database serta output program.</p>
</blockquote>
<h1 id="teori-dan-penjelasan-program">Teori dan Penjelasan Program</h1>
<h2 id="crawling">Crawling</h2>
<p>Crawling merupakan sebuah kegiatan untuk mendapatkan sekelompok data dari web. Data-data tersebut bisa sangat bervariasi, seperti text, citra, audio, video dan lain sebagainya. Dalam program ini, kita akan melakukan crawling pada text, atau lebih dikenal dengan <em>text Mining</em>.</p>
<p>Berikut code untuk melakukan Crawling:</p>
<pre><code class="python">def crawl(src):
    global c
    page = requests.get(src)

    # Mengubah html ke object beautiful soup
    soup = BeautifulSoup(page.content, 'html.parser')

    # Find all item
    items = soup.findAll(class_='article-item')

    #print ('Proses : %.2f' %((c/maxPage)*100) + '%'); c+=1
    for item in items:
        judul = item.find(class_='title-article').getText()
        authors = item.find(class_=&quot;author-article&quot;).findAll(class_='title-author')
        author = ''
        for i in authors: author = author+i.getText()+'; '
        abstrack = item.find(class_='article-abstract').find('p').getText()

        #pengecekan data redundant
        cursor = conn.execute('select * from jurnal2 where judul=?', (judul,))
        cursor = cursor.fetchall()
        if (len(cursor) == 0):
            conn.execute(&quot;INSERT INTO jurnal2 \
                        VALUES (?, ?, ?, ?)&quot;, (judul, author, abstrack, kategori));
</code></pre>

<p><strong>Perlu diingat bahwa, setiap web memiliki struktur html yang berbeda</strong>, maka jika kalian mengubah url web, maka perlu dilakukan penyesuaian pada code.</p>
<p>Selanjutnya, mari kita bahas lebih detail lagi. Untuk mendapatkan tag html .yang diinginkan BeautifulSoup menyediakan 2 fungsi, yaitu :</p>
<ul>
<li>soup.find(parameter)</li>
</ul>
<p>digunakan untuk mendapatkan <em>satu</em> tag html yang muncul pertama kali. Hasilnya berupa objek <code>soup</code></p>
<ul>
<li>soup.findAll(parameter)</li>
</ul>
<p>digunakan untuk mendapatkan <em>semua</em> tag html tersebut. Hasilnya berupa <code>list</code></p>
<p>Sementara itu, untuk parameternya memiliki 3 macam. Kalian bisa mencari berdasarkan:</p>
<ul>
<li>tag html (seperti <code>&lt;p&gt;</code>, <code>&lt;div&gt;</code>, <code>h1</code> dsb). </li>
</ul>
<p>contoh: </p>
<p>code html <code>&lt;div&gt;&lt;p&gt;aku makan sayur&lt;/p&gt;&lt;/div&gt;</code></p>
<p>maka, untuk mendapatkan tag p adalah : <code>soup.find("p")</code></p>
<ul>
<li>class</li>
</ul>
<p>code html <code>&lt;div&gt;&lt;p class='makan'&gt;aku makan sayur&lt;/p&gt;&lt;/div&gt;</code></p>
<p>maka, untuk mendapatkan tag p adalah : <code>soup.find(class_='makan')</code></p>
<ul>
<li>id</li>
</ul>
<p>code html <code>&lt;div&gt;&lt;p id='sayur'&gt;aku makan sayur&lt;/p&gt;&lt;/div&gt;</code></p>
<p>maka, untuk mendapatkan tag p adalah : <code>soup.find(id='sayur')</code></p>
<p>Lalu, untuk mendapatkan textnya, digunakan <code>.getText()</code> pada objek BeautifulSoup.</p>
<p>Setelah kalian paham tentang bagaimana menggunakan library beautifulsoup, kita akan membahas bagaimana code di atas bekerja.</p>
<pre><code class="python">items = soup.findAll(class_='article-item')
</code></pre>

<p>Hal pertama yang kita lakukan adalah menentukan apa saja yang akan kita ambil. Pada kasus jurnal online ini, kita hanya akan mengambil judul, penulis, beserta abstraknya saja. Untuk itu, di sini kita mengambil semua paper, yang bisa kita lihat berada di class 'article-item'. </p>
<pre><code class="python">    for item in items:
        judul = item.find(class_='title-article').getText()
        authors = item.find(class_=&quot;author-article&quot;).findAll(class_='title-author')
        author = ''
        for i in authors: author = author+i.getText()+'; '
        abstrack = item.find(class_='article-abstract').find('p').getText()
</code></pre>

<p>Kemudian, untuk setiap paper, kita ambil judul (dengan class 'title-article'), penulis (dengan class 'author-article'), dan abstrak (dengan class 'article-abstract', lantas dicari tag p).</p>
<pre><code class="python">        #pengecekan data redundant
        cursor = conn.execute('select * from jurnal2 where judul=?', (judul,))
        cursor = cursor.fetchall()
        if (len(cursor) == 0):
            conn.execute(&quot;INSERT INTO jurnal2 \
                        VALUES (?, ?, ?, ?)&quot;, (judul, author, abstrack, kategori));
</code></pre>

<p>Kemudian, memasukkan ke dalam database. Sebelum itu, perlu dilakukan pengecekan apakah ada data yang sama. Karena hal itu bisa mengganggu hasil akhir nanti.</p>
<h2 id="text-extraction">Text-Extraction</h2>
<p>Pada tahap ini, text yang diambil akan dilakukan ekstraksi teks. Terdapat beberapa tahapan, yaitu:</p>
<ul>
<li>Stopword Removal, </li>
</ul>
<p>yaitu menghilangkan kata-kata dan tanda baca yang tidak penting, seperti "dan", "atau", "di", dsb.</p>
<ul>
<li>Stemming</li>
</ul>
<p>yaitu  mengubah suatu  kata menjadi kata dasar, seperti kata "menggunakan" menjadi "guna", "memakan" menajadi "makan".</p>
<ul>
<li>Tokenisasi (n-gram)</li>
</ul>
<p>yaitu memecah kalimat per kata, seperti "aku makan sayur", menjadi "aku", "makan", "sayur". Dalam tokenisasi ini, terdapat variasi jumlah kata yang dipecah. Misal dipecah menjadi 2 suku kata, seperti "aku makan sayur bayam", menjadi "aku makan", "makan sayur", "sayur bayam". Hal tersebut lebih dikenal sebagai <strong>n-gram</strong></p>
<p>Ketiga tahapan di atas sudah saya pecah menjadi dua method berbeda seperti pada code di bawah. </p>
<pre><code class="python">def preprosesing(txt):
    SWfactory = StopWordRemoverFactory()
    stopword = SWfactory.create_stop_word_remover()

    Sfactory = StemmerFactory()
    stemmer = Sfactory.create_stemmer()
    hasil = ''
    for i in txt.split():
        if i.isalpha():
            # Menghilangkan Kata tidak penting
            stop = stopword.remove(i)
            stem = stemmer.stem(stop)
            hasil += stem  + ' '
    return hasil

#VSM
def tokenisasi(txt, ngram=1):
    token = []
    start=0
    end=ngram
    txtSplit = txt.split()
    while end &lt;= len(txtSplit):
        tmp = txtSplit[start:end]
        frase = '' 
        for i in tmp:
            frase += i+ ' ' 
        token.append(frase)
        end+=1; start +=1;
    return token

</code></pre>

<p>Setelah ketiganya dilakukan, maka akan kita bisa membuat sebuah Vector Space Model, dengan menggunakan metode Bag of Words dan TF-IDF.</p>
<h3 id="metode-bag-of-words">Metode Bag of Words</h3>
<p>Bag of Words merupakan salah satu metode untuk membuat sebuah Vector Space Model (VSM) dengan cara menghitung setiap kata pada setiap dokumen. Contohnya seperti ini</p>
<p>doc 1 : "Topi saya bundar"</p>
<p>doc 2 : "Bundar topi saya. Kalau tidak bundar, bukan topi saya"</p>
<p>Kita akan menghitung setiap kata tersebut. Maka didapat VSM sebagai berikut:</p>
<table>
<thead>
<tr>
<th>No Doc</th>
<th align="center">Topi</th>
<th>Saya</th>
<th>Bundar</th>
<th>Kalau</th>
<th align="center">Tidak</th>
<th>Bukan</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td align="center">1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td align="center">0</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td align="center">2</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td align="center">1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Code untuk melakukan perhitungan tersebut adalah sebagai berikut</p>
<pre><code class="python">def countWord(txt, ngram=1):
    '''
        Fungsi ini digunakan untuk menghitung setiap kata pada satu string
    '''
    d = dict()
    token = tokenisasi(txt, ngram)
    for i  in token:
        if d.get(i) == None:
            d[i] = txt.count(i)
    return d
def add_row_VSM(d):
    '''
        Fungsi ini digunakan untuk membangun VSM
    '''
    #init baris baru
    VSM.append([])
    # memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya
    for i in VSM[0]:
        if d.get(i) == None:
            VSM[-1].append(0)
        else :
            VSM[-1].append(d.pop(i));

    # memasukkan kata baru 
    for i in d:
        VSM[0].append(i)  #fitur baru
        for j in range(1, len(VSM)-1):
            #VSM[j].insert(-2,0)
            VSM[j].append(0)
        VSM[-1].append(d.get(i))

</code></pre>

<p>Method counWord digunakan untuk menghitung banyaknya kata pada dokumen. Sementara method add_row_VSM digunakan untuk membuat sebuah matrix VSM tersebut.</p>
<p>Proses pemanggilan fungsi-fungsi di atas dilakukan oleh code di bawah ini:</p>
<pre><code class="python">cursor = conn.execute(&quot;SELECT * from jurnal2&quot;)
cursor = cursor.fetchall()
cursor = cursor[:60]
pertama = True
corpus = list()
label = list()
c=1
n = int(input(&quot;ngram : &quot;))
#n=1
for row in cursor:
    print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1
    label.append(row[-1])
    txt = row[-2]
    cleaned = preprosesing(txt)
    cleaned = cleaned[:-1]
    corpus.append(cleaned)

    d = countWord(cleaned, n)
    if pertama:
        pertama = False
        VSM = list((list(), list()))
        for key in d:
            VSM[0].append(key)
            VSM[1].append(d[key])
    else:
        add_row_VSM(d)
</code></pre>

<p>Lalu untuk menampilkan hasilnya (agar terlihat rapi) maka kita export ke csv. Serta kita pisahkan antara kata-kata dengan nilainya:</p>
<pre><code class="python">write_csv(&quot;bow_manual_%d.csv&quot;%n, VSM)
feature_name = VSM[0]
bow = np.array(VSM[1:])
</code></pre>

<h3 id="tf-idf">TF-IDF</h3>
<p>Selain menggunakan Bag of Words, kita juga bisa menggunakan metode TF-IDF. Hal ini karena Bag of Word memiliki kelemahan tersendiri. </p>
<p>TF-IDF sendiri merupakan kepanjangan dari Term Frequence (frekuensi Kata) dan Invers Document Frequence (invers frekuensi Dokumen). Rumus TF-IDF sendiri terbilang mudah karena hanya TFxIDF.</p>
<p>Kita telah mencari TF sebelumnya (yaitu Bag of Words), karena konsep keduanya yang memang sama. Sekarang kita tinggal mencari nilai IDF.</p>
<p>Untuk mendapatkan IDF, pertama kita perlu mencari DF (frekuensi Dokumen). Misalnya:</p>
<p>doc1 : Topi Saya Bundar, bundar topi saya</p>
<p>doc2 : Matahari itu terlihat bundar</p>
<p>Maka, bisa kita ketahui:</p>
<table>
<thead>
<tr>
<th>Kata</th>
<th>Jumlah Dokumen yang memiliki kata tersebut (DF)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Topi</td>
<td>1</td>
</tr>
<tr>
<td>Saya</td>
<td>1</td>
</tr>
<tr>
<td>Bundar</td>
<td>2</td>
</tr>
<tr>
<td>Matahari</td>
<td>1</td>
</tr>
<tr>
<td>itu</td>
<td>1</td>
</tr>
<tr>
<td>terlihat</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Setelah itu, kita lakukan invers pada setiap kata:</p>
<p><img alt="rumus Shilhouette" src="../img/rumus_tfidf.PNG" /></p>
<p>Setelah ketemu, maka tinggal kita kalikan TFxIDF. Berikut code programnya:</p>
<p><img alt="rumus Shilhouette" src="../img/rumus_tfidf2.PNG" /></p>
<pre><code class="python">df = list()
total_doc = bow.shape[0]
for kolom in range(len(bow[0])):
    total = 0
    for baris in range(len(bow)):
        if (bow[baris, kolom] &gt; 0):
            total +=1
    df.append(total)
df = np.array(df)

idf = list()
for i in df:
    tmp = 1 + log10(total_doc/(1+i))
    idf.append(tmp)
idf = np.array(idf)

tfidf = bow * idf

</code></pre>

<h2 id="preprocessing">Preprocessing</h2>
<p>Tahap <em>preprocessing</em> merupakan tahap mengolah data agar data lebih mudah diproses. Ada banyak jenis dari <em>preprocessing</em>. Namun, di kesempatan kali ini kita hanya akan membahas tentang seleksi fitur.</p>
<h3 id="seleksi-fitur">Seleksi Fitur</h3>
<p>Seleksi fitur merupakan salah satu cara untuk mengurangi dimensi fitur yang sangat banyak. Seperti pada kasus kita, Text Mining, jumlah fitur yang didapatkan bisa mencapai lebih dari 2000 kata yang berbeda. Namun, tidak semua kata tersebut benar-benar berpengaruh pada hasil akhir nantinya. </p>
<p>Selain itu, kita tahu bahwa semakin banyak data yang diproses, maka lebih banyak biaya dan waktu yang digunakan untuk memprosesnya. Oleh karena itu,  kita perlu melakukan pengurangan fitur tanpa mengurangi kualitas hasil akhir, misalnya dengan Seleksi Fitur.</p>
<p>Pada dasarnya, seleksi fitur memiliki 3 tipe umum:</p>
<ol>
<li>Wrap</li>
<li>Filter</li>
<li>Embed</li>
</ol>
<p>Selain itu, Seleksi Fitur juga memiliki banyak sekali metode-metode, seperti Information Gain, Chi Square, Pearson, dll.</p>
<h4 id="pearson-correlation">Pearson Correlation</h4>
<p>Pendekatan Pearson merupakan pendekatan paling sederhana. Pada pendekatan ini, setiap fitur akan dihitung korelasinya. Semakin tinggi nilainya, maka fitur tersebut semakin kuat korelasinya. Lalu fitur yang memiliki korelasi tinggi akan dibuang salah satunya.</p>
<p><strong>Pendekatan ini digunakan untuk data tipe numerik</strong>.</p>
<p>Codenya sebagai berikut:</p>
<pre><code class="python">def pearsonCalculate(data, u,v):
    &quot;i, j is an index&quot;
    atas=0; bawah_kiri=0; bawah_kanan = 0
    for k in range(len(data)):
        atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v])
        bawah_kiri += (data[k,u] - meanFitur[u])**2
        bawah_kanan += (data[k,v] - meanFitur[v])**2
    bawah_kiri = bawah_kiri ** 0.5
    bawah_kanan = bawah_kanan ** 0.5
    return atas/(bawah_kiri * bawah_kanan)
def meanF(data):
    meanFitur=[]
    for i in range(len(data[0])):
        meanFitur.append(sum(data[:,i])/len(data))
    return np.array(meanFitur)
def seleksiFiturPearson(data, threshold):
    global meanFitur
    meanFitur = meanF(data)
    u=0
    while u &lt; len(data[0]):
        dataBaru=data[:, :u+1]
        meanBaru=meanFitur[:u+1]
        v = u
        while v &lt; len(data[0]):
            if u != v:
                value = pearsonCalculate(data, u,v)
                if value &lt; threshold:
                    dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1)))
                    meanBaru = np.hstack((meanBaru, meanFitur[v]))
            v+=1
        data = dataBaru
        meanFitur=meanBaru
        if u%50 == 0 : print(u, data.shape)
        u+=1
    return data
</code></pre>

<h4 id="chi-square">Chi Square</h4>
<p>Sama seperti pendekatan pearson, hanya saja, pendekatan ini lebih digunakan untuk data tipe categorical.</p>
<h2 id="procesing">Procesing</h2>
<p>Terdapat dua hal yang bisa kita lakukan pada sebuah data. </p>
<ol>
<li>Clustering (pengelompokan)</li>
<li>Klasifikasi (fokus pada hasil prediksi)</li>
</ol>
<h3 id="clustering">Clustering</h3>
<p>Clustering merupakan pengelompokan data menjadi k-kelompok (dengan k merupakan banyak kelompok). Pengelompkan tersebut berdasarkan ciri yang mirip. Pada kasus ini, maka ciri yang mirip bisa diketahui dari kata yang menjadi ciri dari setiap dokumen.</p>
<p>Metode Clustering sendiri ada banyak. Salah duanya adalah K-Means Clustering dan Fuzzy C-Means Clustering.</p>
<p>Setelah dilakukan proses Clustering, perlu kita cari nilai Silhouette Coefficient untuk melihat apakah hasil cluster tersebut sudah bagus atau tidak.</p>
<h4 id="k-means">K-Means</h4>
<pre><code class="python"># Clustering
kmeans = KMeans(n_clusters=5, random_state=0).fit(tfidf_matrix.todense())

for i in range(len(kmeans.labels_)):
    print(&quot;Doc %d =&gt;&gt; cluster %d&quot; %(i+1, kmeans.labels_[i]))
</code></pre>

<p>Code di atas adalah code untuk melakukan clustering. Pada contoh ini cluster dibagi menjadi 5. Banyak cluster bisa diubah sesuai kebutuhan.</p>
<h4 id="fuzzy-c-means">Fuzzy C-Means</h4>
<pre><code class="python">cntr, u, u0, distant, fObj, iterasi, fpc = fuzz.cmeans(tfidf.T, 3, 2, 0.00001, 1000)
membership = np.argmax(u, axis=0)
</code></pre>

<p>parameter dari fuzzy c-means berturut-turut : data, jumlahCluster, pembobot, erorMaksimal, serta IterasiMaksimal.</p>
<h4 id="shilhouette-coefisient">Shilhouette Coefisient</h4>
<p>Shilhouette Coefisient merupakan salah satu metode evaluasi yang digunakan untuk model Cluster, seperti K-Means atau Fuzzy C-Means. Metode ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan.  Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus <em>EuclideanDistance</em>. Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut :</p>
<ol>
<li>
<p>Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan  seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai  rata-rata yang disebut a<em>i</em>.</p>
</li>
<li>
<p>Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan  objek yang berada di cluster lainnya. Dari semua jarak rata-rata  tersebut ambil nilai yang paling kecil. Nilai ini disebut b<em>i</em>.</p>
</li>
<li>
<p>Setelah itu maka untuk objek i memiliki nilai <em>silhoutte coefisien</em> :</p>
</li>
</ol>
<p><img alt="rumus Shilhouette" src="../img/rumus_shilhouette.PNG" /></p>
<p>Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif. Maka dapat dikatakan, jika s<em>i</em> = 1 berarti objek <em>i</em> sudah berada dalam cluster yang tepat. Jika nilai s<em>i</em> = 0 maka objek <em>i</em> berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s<em>i</em> = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek <em>i</em> lebih tepat dimasukan ke dalam cluster yang lain. </p>
<p>Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut.</p>
<h1 id="analisis">Analisis</h1>
<p>Data yang digunakan merupakan data dari <a href="https://garuda.ristekdikti.go.id">Jurnal Online</a> berupa paper dari 9 jurnal berbeda, dengan masing-masing jurnal didapat 10 paper. Total data yang digunakan sebanyak 90 judul dan abstrak. </p>
<p>Pada percobaan ini digunakan model Fuzzy C-Means, dengan percobaan cluster dilakukan berulang kali dengan mengubah nilai parameter, yaitu Threshold untuk Seleksi Fitur, Jumlah Cluster, serta banyak N-gram. Maka dihasilkan nilai Shilhouette Coeffisient sebagai Berikut:</p>
<h2 id="tanpa-dilakukan-preprocessing">Tanpa dilakukan preprocessing</h2>
<p>Pada percobaan ini, data tidak dilakukan Seleksi Fitur apapun. Data langsung dibuat model Cluster menggunakan Fuzzy C-Means. Percobaan dilakukan beberapa kali dengan mengubah jumlah Cluster, dengan rentang 2-5.</p>
<table>
<thead>
<tr>
<th>Jumlah Cluster</th>
<th>Shilhouette Coeffisien</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>0.325</td>
</tr>
<tr>
<td>3</td>
<td>0.325</td>
</tr>
<tr>
<td>4</td>
<td>0.325</td>
</tr>
<tr>
<td>5</td>
<td>0.325</td>
</tr>
</tbody>
</table>
<p>Dari hasil di atas, dapat disimpulkan, apabila tidak dilakukan tahap Seleksi Fitur, banyak cluster tidak berpengaruh pada hasil cluster tersebut. </p>
<h2 id="berdasarkan-jumlah-cluster">Berdasarkan Jumlah Cluster</h2>
<p>Jumlah Cluster berpengaruh dalam menentukan sebanyak apa kelompok yang akan dibuat. Hal ini diharapkan kita bisa menentukan Jumlah Cluster yang paling tepat dalam menentukan model untuk kasus ini.</p>
<p>Pada percobaan ini, nilai Threshold pada seleksi Fitur <em>Pearson</em> sebesar 0.8. Percobaan dilakukan sebanyak 9 kali, dengan mengubah parameter <em>c</em> pada C-Means dengan rentang 2-5. Hasilnya sebagai berikut:</p>
<table>
<thead>
<tr>
<th>Jumlah Cluster</th>
<th>Shilhouette Coeffisient</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>0.199</td>
</tr>
<tr>
<td>3</td>
<td>0.057</td>
</tr>
<tr>
<td>4</td>
<td>0.093</td>
</tr>
<tr>
<td>5</td>
<td>0.050</td>
</tr>
</tbody>
</table>
<p>Dari sini bisa kita melihat bahwa <em>jumlah cluster 2</em> memiliki nilai coefisien tertinggi dengan nilai Shilhouette 0.199. Sebaliknya, cluster yang paling tidak tepat, apabila jumlah cluster sebanyak 5 dengan nilai shilhouette 0.050.</p>
<h2 id="berdasarkan-nilai-threshold">Berdasarkan nilai Threshold</h2>
<p>Nilai Threshold berfungsi untuk menentukan seberapa banyak fitur-fitur yang akan dihapus. Dengan dilakukan percobaan ini, diharapkan bisa menentukan nilai Threshold yang paling cocok. </p>
<p>Nilai Threshold yang digunakan merupakan {0.9, 0.85, 0.8, 0.75, 0.7}. Melihat Percobaan sebelumnya, Jumlah Cluster yang paling baik merupakan 2, maka, digunakan C=2. Hasilnya didapatkan :</p>
<table>
<thead>
<tr>
<th>Nilai Threshold</th>
<th>Shilhouette Coeffisient</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.9</td>
<td>0.334</td>
</tr>
<tr>
<td>0.85</td>
<td>0.342</td>
</tr>
<tr>
<td>0.8</td>
<td>0.199</td>
</tr>
<tr>
<td>0.75</td>
<td>0.255</td>
</tr>
<tr>
<td>0.7</td>
<td>0.282</td>
</tr>
</tbody>
</table>
<p>Nilai Shilhouette paling tinggi didapat ketika nilai Threshold 0.85, yaitu 0.342. Selanjutnya, secara berturut-turut, 0.9, 0.7, 0.75, dan nilai terendah 0.8, yaitu 0.199.</p>
<h2 id="berdasarkan-n-gram">Berdasarkan N-Gram</h2>
<p>N-Gram berfungsi untuk menentukan banyaknya kata per tokenisasi.  Hal ini diharapkan untuk mengetahui N-gram terbaik.</p>
<p>Pada percobaan ini, digunakan jumlah cluster sebanyak 2. Percobaan dilakukan sebanyak 3 kali dengan mengubah parameter "n" pada N-gram. Maka didapat:</p>
<table>
<thead>
<tr>
<th>n-gram</th>
<th>Shilhouette Coeffisient</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.325</td>
</tr>
<tr>
<td>2</td>
<td>0.176</td>
</tr>
<tr>
<td>3</td>
<td>0.205</td>
</tr>
</tbody>
</table>
<p>Dari hasil di atas, didapat nilai shilhouette tertinggi dengan n-gram 1, yaitu 0.325. Sementara paling rendah adalah 0.176, apabila nilai n-gram adalah 2. </p>
<p>Selain itu, apabila dilakukan seleksi fitur, <code>ngram &gt; 1</code>  akan menghasilkan 1 cluster, tidak peduli berapapun jumlah cluster yg ditentukan.</p>
<h2 id="k-means_1">K-Means</h2>
<p>Karena nilai Shilhouette yang relatif kecil, hasil dari Fuzzy C-Means akan dibandingkan dengan metode lain, yaitu K-Means, untuk melihat manakah yang lebih baik. </p>
<p>Parameter yang digunakan di antaranya, Jumlah Cluster = 2, Threshold = 0.85, dan N-Gram = 1. Maka, diperoleh hasil berikut:</p>
<table>
<thead>
<tr>
<th>Metode</th>
<th>Shilhouette Coefisient</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fuzzy C-Means</td>
<td>0.342</td>
</tr>
<tr>
<td>K-Means</td>
<td>0.430</td>
</tr>
</tbody>
</table>
<h1 id="kesimpulan">Kesimpulan</h1>
<p>Dari hasil percobaan di atas, dapat disimpulkan bahwa metode yang terbaik adalah K-Means, dengan parameter, n-gram=1, jumlah cluster=2, dan nilai Treshold=0.85.</p></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
