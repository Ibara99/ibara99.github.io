{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Simple Web Crawler \u00b6 \u200b Program ini digunakan untuk mengekstrak data dari sebuah website. Untuk source code lengkap, sila klik di sini . Pengantar \u00b6 Ucapan terima kasih kepada Bapak Mulaab, S.Si., M.Kom. selaku Dosen Pembimbing kami Nama : Ibnu Asro Putra NRP : 160411100023 Mata kuliah : Penambangan dan Pencarian Web - 2019 Jurusan : Teknik Informatika Perguruan Tinggi : Universitas Trunojoyo Madura Environment \u00b6 Program ini dijalankan menggunakan: Bahasa Python, dengan library: BeautifulSoup4 (install menggunakan pip) requests (install menggunakan pip) SQLite3 (library bawaan python) csv (library bawaan python) numpy (install menggunakan pip) scipy (install menggunakan pip) scikit-learn (install menggunakan pip, perlu untuk install numpy dan scipy terlebih dulu) Scikit-fuzzy (install menggunakan pip, perlu untuk install numpy dan scipy terlebih dulu) Website target : Jurnal Online ) Jika anda memutuskan untuk meng-update data, maka Program hanya bisa dijalankan menggunakan Internet, atau program akan error Setiap kali program dijalankan, akan muncul file baru bernama test.db , dan beberapa file csv . File tersebut merupakan file database serta output program.","title":"Home"},{"location":"#simple-web-crawler","text":"\u200b Program ini digunakan untuk mengekstrak data dari sebuah website. Untuk source code lengkap, sila klik di sini .","title":"Simple Web Crawler"},{"location":"#pengantar","text":"Ucapan terima kasih kepada Bapak Mulaab, S.Si., M.Kom. selaku Dosen Pembimbing kami Nama : Ibnu Asro Putra NRP : 160411100023 Mata kuliah : Penambangan dan Pencarian Web - 2019 Jurusan : Teknik Informatika Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Pengantar"},{"location":"#environment","text":"Program ini dijalankan menggunakan: Bahasa Python, dengan library: BeautifulSoup4 (install menggunakan pip) requests (install menggunakan pip) SQLite3 (library bawaan python) csv (library bawaan python) numpy (install menggunakan pip) scipy (install menggunakan pip) scikit-learn (install menggunakan pip, perlu untuk install numpy dan scipy terlebih dulu) Scikit-fuzzy (install menggunakan pip, perlu untuk install numpy dan scipy terlebih dulu) Website target : Jurnal Online ) Jika anda memutuskan untuk meng-update data, maka Program hanya bisa dijalankan menggunakan Internet, atau program akan error Setiap kali program dijalankan, akan muncul file baru bernama test.db , dan beberapa file csv . File tersebut merupakan file database serta output program.","title":"Environment"},{"location":"Clustering/","text":"Clustering merupakan pengelompokan data menjadi k-kelompok (dengan k merupakan banyak kelompok). Pengelompokan tersebut berdasarkan ciri yang mirip. Pada kasus ini, maka ciri yang mirip bisa diketahui dari kata yang menjadi ciri dari setiap dokumen. Metode Clustering sendiri ada banyak. Salah duanya adalah K-Means Clustering dan Fuzzy C-Means Clustering. Setelah dilakukan proses Clustering, perlu kita cari nilai Silhouette Coefficient untuk melihat apakah hasil cluster tersebut sudah bagus atau tidak. K-Means \u00b6 K-means merupakan salah satu metode pengelompokan data nonhierarki (sekatan) yang berusaha mempartisi data yang ada ke dalam bentuk dua atau lebih kelompok. Metode ini mempartisi data ke dalam kelompok sehingga data berkarakteristik sama dimasukkan ke dalam satu kelompok yang sama dan data yang berkarakteristik berbeda dikelompokkan kedalam kelompok yang lain. Adapun tujuan pengelompokkan data ini adalah untuk meminimalkan fungsi objektif yang diatur dalam proses pengelompokan, yang pada umumnya berusaha meminimalkan variasi di dalam suatu kelompok dan memaksimalkan variasi antar kelompok 1 Algoritma K-Means adalah: Tentukan jumlah klaster yang ingin dibentuk dan tetapkan pusat cluster k. Menggunakan jarak euclidean kemudian hitung setiap data ke pusat cluster $$ \\mathrm{d}(\\mathbf{i}, \\mathbf{k})=\\sqrt{\\sum_{i}^{m}\\left(C_{i} j-C_{k} j\\right)^{2}} $$ Kelompokkan data ke dalam cluster dengan jarak yang paling pendek dengan persamaan $$ \\min \\sum_{k}^{i}-a_{i k}-=\\sqrt{\\sum_{i}^{m}\\left(C_{i} j-C_{k} j\\right)^{2}} $$ Hitung pusat cluster yang baru menggunakan persamaan $$ C_{k j}=\\frac{\\sum_{k}^{i} x_{i j}}{p} $$ Dengan : Xij\u000f Kluster ke k p = banyaknya anggota kluster ke - k Ulangi langkah dua sampai dengan empat sehingga sudah tidak ada lagi data yang berpindah ke kluster yang lain Di sini kita akan menggunakan Library scikit-learn untuk memudahkan. Berikut codenya: from sklearn.cluster import KMeans # Clustering kmeans = KMeans ( n_clusters = 5 , random_state = 0 ) . fit ( tfidf_matrix . todense ()) for i in range ( len ( kmeans . labels_ )): print ( \"Doc %d =>> cluster %d \" % ( i + 1 , kmeans . labels_ [ i ])) Code di atas adalah code untuk melakukan clustering. Pada contoh ini cluster dibagi menjadi 5. Banyak cluster bisa diubah sesuai kebutuhan. Fuzzy C-Means \u00b6 Konsep dasar Metode C-Means Clustering yaitu untuk menentukan pusat cluster. yang akan menandai lokasi untuk setiap cluster, dengan cara memperbaiki pusat cluster sehingga terbentuk suatu cluster baru. 2 Untuk melihat perhitungan manualnya, silakan klik di sini . Berikut merupakan code untuk melakukan Fuzzy C-Means menggunakan Library scikit-fuzzy import skfuzzy as fuzz cntr , u , u0 , distant , fObj , iterasi , fpc = fuzz . cmeans ( tfidf . T , 3 , 2 , 0.00001 , 1000 ) membership = np . argmax ( u , axis = 0 ) parameter dari fuzzy c-means berturut-turut : data, jumlahCluster, pembobot, erorMaksimal, serta IterasiMaksimal. Shilhouette Coefisient \u00b6 Shilhouette Coefisient merupakan salah satu metode evaluasi yang digunakan untuk model Cluster, seperti K-Means atau Fuzzy C-Means. Metode ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan 3 . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a*i*. Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b*i*. Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : \\mathbf{S}_{i}=\\frac{\\left(\\mathbf{b}_{i}-\\mathbf{a}_{i}\\right)}{\\max \\left(\\mathbf{a}_{i}, \\mathbf{b}_{i}\\right)} \\mathbf{S}_{i}=\\frac{\\left(\\mathbf{b}_{i}-\\mathbf{a}_{i}\\right)}{\\max \\left(\\mathbf{a}_{i}, \\mathbf{b}_{i}\\right)} Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif. Maka dapat dikatakan, jika s*i* = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s*i* = 0 maka objek i berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s*i* = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut. Sourcecode untuk Shilhouette menggunakan Library scikit-learn adalah sebagai berikut: from sklearn.metrics import silhouette_score s_avg = silhouette_score ( xBaru2 , membership , random_state = 10 ) dengan parameter berturut-turut: data, hasil_cluster, dan random_state. Maulida, Linda. \"PENERAPAN DATAMINING DALAM MENGELOMPOKKAN KUNJUNGAN WISATAWAN KE OBJEK WISATA UNGGULAN DI PROV. DKI JAKARTA DENGAN K-MEANS.\" JISKA (Jurnal Informatika Sunan Kalijaga) 2.3 (2018): 167-174. \u21a9 http://www.infoseribucara.com/algoritma/algoritma-c-means-clustering-demo-program.html \u21a9 https://lookmylife.wordpress.com/2011/10/03/metode-silhoutte-coeffisien/ \u21a9","title":"Clustering"},{"location":"Clustering/#k-means","text":"K-means merupakan salah satu metode pengelompokan data nonhierarki (sekatan) yang berusaha mempartisi data yang ada ke dalam bentuk dua atau lebih kelompok. Metode ini mempartisi data ke dalam kelompok sehingga data berkarakteristik sama dimasukkan ke dalam satu kelompok yang sama dan data yang berkarakteristik berbeda dikelompokkan kedalam kelompok yang lain. Adapun tujuan pengelompokkan data ini adalah untuk meminimalkan fungsi objektif yang diatur dalam proses pengelompokan, yang pada umumnya berusaha meminimalkan variasi di dalam suatu kelompok dan memaksimalkan variasi antar kelompok 1 Algoritma K-Means adalah: Tentukan jumlah klaster yang ingin dibentuk dan tetapkan pusat cluster k. Menggunakan jarak euclidean kemudian hitung setiap data ke pusat cluster $$ \\mathrm{d}(\\mathbf{i}, \\mathbf{k})=\\sqrt{\\sum_{i}^{m}\\left(C_{i} j-C_{k} j\\right)^{2}} $$ Kelompokkan data ke dalam cluster dengan jarak yang paling pendek dengan persamaan $$ \\min \\sum_{k}^{i}-a_{i k}-=\\sqrt{\\sum_{i}^{m}\\left(C_{i} j-C_{k} j\\right)^{2}} $$ Hitung pusat cluster yang baru menggunakan persamaan $$ C_{k j}=\\frac{\\sum_{k}^{i} x_{i j}}{p} $$ Dengan : Xij\u000f Kluster ke k p = banyaknya anggota kluster ke - k Ulangi langkah dua sampai dengan empat sehingga sudah tidak ada lagi data yang berpindah ke kluster yang lain Di sini kita akan menggunakan Library scikit-learn untuk memudahkan. Berikut codenya: from sklearn.cluster import KMeans # Clustering kmeans = KMeans ( n_clusters = 5 , random_state = 0 ) . fit ( tfidf_matrix . todense ()) for i in range ( len ( kmeans . labels_ )): print ( \"Doc %d =>> cluster %d \" % ( i + 1 , kmeans . labels_ [ i ])) Code di atas adalah code untuk melakukan clustering. Pada contoh ini cluster dibagi menjadi 5. Banyak cluster bisa diubah sesuai kebutuhan.","title":"K-Means"},{"location":"Clustering/#fuzzy-c-means","text":"Konsep dasar Metode C-Means Clustering yaitu untuk menentukan pusat cluster. yang akan menandai lokasi untuk setiap cluster, dengan cara memperbaiki pusat cluster sehingga terbentuk suatu cluster baru. 2 Untuk melihat perhitungan manualnya, silakan klik di sini . Berikut merupakan code untuk melakukan Fuzzy C-Means menggunakan Library scikit-fuzzy import skfuzzy as fuzz cntr , u , u0 , distant , fObj , iterasi , fpc = fuzz . cmeans ( tfidf . T , 3 , 2 , 0.00001 , 1000 ) membership = np . argmax ( u , axis = 0 ) parameter dari fuzzy c-means berturut-turut : data, jumlahCluster, pembobot, erorMaksimal, serta IterasiMaksimal.","title":"Fuzzy C-Means"},{"location":"Clustering/#shilhouette-coefisient","text":"Shilhouette Coefisient merupakan salah satu metode evaluasi yang digunakan untuk model Cluster, seperti K-Means atau Fuzzy C-Means. Metode ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan 3 . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a*i*. Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b*i*. Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : \\mathbf{S}_{i}=\\frac{\\left(\\mathbf{b}_{i}-\\mathbf{a}_{i}\\right)}{\\max \\left(\\mathbf{a}_{i}, \\mathbf{b}_{i}\\right)} \\mathbf{S}_{i}=\\frac{\\left(\\mathbf{b}_{i}-\\mathbf{a}_{i}\\right)}{\\max \\left(\\mathbf{a}_{i}, \\mathbf{b}_{i}\\right)} Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif. Maka dapat dikatakan, jika s*i* = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s*i* = 0 maka objek i berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s*i* = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut. Sourcecode untuk Shilhouette menggunakan Library scikit-learn adalah sebagai berikut: from sklearn.metrics import silhouette_score s_avg = silhouette_score ( xBaru2 , membership , random_state = 10 ) dengan parameter berturut-turut: data, hasil_cluster, dan random_state. Maulida, Linda. \"PENERAPAN DATAMINING DALAM MENGELOMPOKKAN KUNJUNGAN WISATAWAN KE OBJEK WISATA UNGGULAN DI PROV. DKI JAKARTA DENGAN K-MEANS.\" JISKA (Jurnal Informatika Sunan Kalijaga) 2.3 (2018): 167-174. \u21a9 http://www.infoseribucara.com/algoritma/algoritma-c-means-clustering-demo-program.html \u21a9 https://lookmylife.wordpress.com/2011/10/03/metode-silhoutte-coeffisien/ \u21a9","title":"Shilhouette Coefisient"},{"location":"Evaluasi/","text":"Evaluasi \u00b6 Data yang digunakan merupakan data dari Jurnal Online berupa paper dari 9 jurnal berbeda, dengan masing-masing jurnal didapat 10 paper. Total data yang digunakan sebanyak 90 judul dan abstrak. Pada percobaan ini digunakan model Fuzzy C-Means, dengan percobaan cluster dilakukan berulang kali dengan mengubah nilai parameter, yaitu Threshold untuk Seleksi Fitur, Jumlah Cluster, serta banyak N-gram. Maka dihasilkan nilai Shilhouette Coeffisient sebagai Berikut: Tanpa dilakukan preprocessing \u00b6 Pada percobaan ini, data tidak dilakukan Seleksi Fitur apapun. Data langsung dibuat model Cluster menggunakan Fuzzy C-Means. Percobaan dilakukan beberapa kali dengan mengubah jumlah Cluster, dengan rentang 2-5. Jumlah Cluster Shilhouette Coeffisien 2 0.325 3 0.325 4 0.325 5 0.325 Dari hasil di atas, dapat disimpulkan, apabila tidak dilakukan tahap Seleksi Fitur, banyak cluster tidak berpengaruh pada hasil cluster tersebut. Berdasarkan Jumlah Cluster \u00b6 Jumlah Cluster berpengaruh dalam menentukan sebanyak apa kelompok yang akan dibuat. Hal ini diharapkan kita bisa menentukan Jumlah Cluster yang paling tepat dalam menentukan model untuk kasus ini. Pada percobaan ini, nilai Threshold pada seleksi Fitur Pearson sebesar 0.8. Percobaan dilakukan sebanyak 9 kali, dengan mengubah parameter c pada C-Means dengan rentang 2-5. Hasilnya sebagai berikut: Jumlah Cluster Shilhouette Coeffisient 2 0.199 3 0.057 4 0.093 5 0.050 Dari sini bisa kita melihat bahwa jumlah cluster 2 memiliki nilai coefisien tertinggi dengan nilai Shilhouette 0.199. Sebaliknya, cluster yang paling tidak tepat, apabila jumlah cluster sebanyak 5 dengan nilai shilhouette 0.050. Berdasarkan nilai Threshold \u00b6 Nilai Threshold berfungsi untuk menentukan seberapa banyak fitur-fitur yang akan dihapus. Dengan dilakukan percobaan ini, diharapkan bisa menentukan nilai Threshold yang paling cocok. Nilai Threshold yang digunakan merupakan {0.9, 0.85, 0.8, 0.75, 0.7}. Melihat Percobaan sebelumnya, Jumlah Cluster yang paling baik merupakan 2, maka, digunakan C=2. Hasilnya didapatkan : Nilai Threshold Shilhouette Coeffisient 0.9 0.334 0.85 0.342 0.8 0.199 0.75 0.255 0.7 0.282 Nilai Shilhouette paling tinggi didapat ketika nilai Threshold 0.85, yaitu 0.342. Selanjutnya, secara berturut-turut, 0.9, 0.7, 0.75, dan nilai terendah 0.8, yaitu 0.199. Berdasarkan N-Gram \u00b6 N-Gram berfungsi untuk menentukan banyaknya kata per tokenisasi. Hal ini diharapkan untuk mengetahui N-gram terbaik. Pada percobaan ini, digunakan jumlah cluster sebanyak 2. Percobaan dilakukan sebanyak 3 kali dengan mengubah parameter \"n\" pada N-gram. Maka didapat: n-gram Shilhouette Coeffisient 1 0.325 2 0.176 3 0.205 Dari hasil di atas, didapat nilai shilhouette tertinggi dengan n-gram 1, yaitu 0.325. Sementara paling rendah adalah 0.176, apabila nilai n-gram adalah 2. Selain itu, apabila dilakukan seleksi fitur, ngram > 1 akan menghasilkan 1 cluster, tidak peduli berapapun jumlah cluster yg ditentukan. K-Means \u00b6 Karena nilai Shilhouette yang relatif kecil, hasil dari Fuzzy C-Means akan dibandingkan dengan metode lain, yaitu K-Means, untuk melihat manakah yang lebih baik. Parameter yang digunakan di antaranya, Jumlah Cluster = 2, Threshold = 0.85, dan N-Gram = 1. Maka, diperoleh hasil berikut: Metode Shilhouette Coefisient Fuzzy C-Means 0.342 K-Means 0.430 Kesimpulan \u00b6 Dari hasil percobaan di atas, dapat disimpulkan bahwa metode yang terbaik adalah K-Means, dengan parameter, n-gram=1, jumlah cluster=2, dan nilai Treshold=0.85. Source Code dan Output \u00b6 Source code lengkap dapat dilihat di sini . Source code hanya berjalan dengan parameter: N-gram => input user saat running Nilai Threshold untuk Seleksi Fitur: {0.9, 0.8} Cluster => 3 Cluster Untuk melakukan pengujian diatas, dilakukan pengubahan parameter secara manual berkali-kali. Salah satu hasil running bisa dilihat di repository dengan tipe file .csv . Berikut detail outputnya: test.db => berisi data yang telah dicrawl (format: SQLite ) bow_manual_1.csv => berisi hasil dari metode bag of word tfidf1.csv => berisi hasil dari metode tfidf seleksiFitur1.csv => berisi hasil dari metode Seleksi Fitur Cluster1.csv => berisi hasil dari Clustering tanpa dilakukan Seleksi Fitur Cluster1FS.csv => berisi hasil dari Clustering dengan dilakukan Seleksi Fitur","title":"Evaluasi"},{"location":"Evaluasi/#evaluasi","text":"Data yang digunakan merupakan data dari Jurnal Online berupa paper dari 9 jurnal berbeda, dengan masing-masing jurnal didapat 10 paper. Total data yang digunakan sebanyak 90 judul dan abstrak. Pada percobaan ini digunakan model Fuzzy C-Means, dengan percobaan cluster dilakukan berulang kali dengan mengubah nilai parameter, yaitu Threshold untuk Seleksi Fitur, Jumlah Cluster, serta banyak N-gram. Maka dihasilkan nilai Shilhouette Coeffisient sebagai Berikut:","title":"Evaluasi"},{"location":"Evaluasi/#tanpa-dilakukan-preprocessing","text":"Pada percobaan ini, data tidak dilakukan Seleksi Fitur apapun. Data langsung dibuat model Cluster menggunakan Fuzzy C-Means. Percobaan dilakukan beberapa kali dengan mengubah jumlah Cluster, dengan rentang 2-5. Jumlah Cluster Shilhouette Coeffisien 2 0.325 3 0.325 4 0.325 5 0.325 Dari hasil di atas, dapat disimpulkan, apabila tidak dilakukan tahap Seleksi Fitur, banyak cluster tidak berpengaruh pada hasil cluster tersebut.","title":"Tanpa dilakukan preprocessing"},{"location":"Evaluasi/#berdasarkan-jumlah-cluster","text":"Jumlah Cluster berpengaruh dalam menentukan sebanyak apa kelompok yang akan dibuat. Hal ini diharapkan kita bisa menentukan Jumlah Cluster yang paling tepat dalam menentukan model untuk kasus ini. Pada percobaan ini, nilai Threshold pada seleksi Fitur Pearson sebesar 0.8. Percobaan dilakukan sebanyak 9 kali, dengan mengubah parameter c pada C-Means dengan rentang 2-5. Hasilnya sebagai berikut: Jumlah Cluster Shilhouette Coeffisient 2 0.199 3 0.057 4 0.093 5 0.050 Dari sini bisa kita melihat bahwa jumlah cluster 2 memiliki nilai coefisien tertinggi dengan nilai Shilhouette 0.199. Sebaliknya, cluster yang paling tidak tepat, apabila jumlah cluster sebanyak 5 dengan nilai shilhouette 0.050.","title":"Berdasarkan Jumlah Cluster"},{"location":"Evaluasi/#berdasarkan-nilai-threshold","text":"Nilai Threshold berfungsi untuk menentukan seberapa banyak fitur-fitur yang akan dihapus. Dengan dilakukan percobaan ini, diharapkan bisa menentukan nilai Threshold yang paling cocok. Nilai Threshold yang digunakan merupakan {0.9, 0.85, 0.8, 0.75, 0.7}. Melihat Percobaan sebelumnya, Jumlah Cluster yang paling baik merupakan 2, maka, digunakan C=2. Hasilnya didapatkan : Nilai Threshold Shilhouette Coeffisient 0.9 0.334 0.85 0.342 0.8 0.199 0.75 0.255 0.7 0.282 Nilai Shilhouette paling tinggi didapat ketika nilai Threshold 0.85, yaitu 0.342. Selanjutnya, secara berturut-turut, 0.9, 0.7, 0.75, dan nilai terendah 0.8, yaitu 0.199.","title":"Berdasarkan nilai Threshold"},{"location":"Evaluasi/#berdasarkan-n-gram","text":"N-Gram berfungsi untuk menentukan banyaknya kata per tokenisasi. Hal ini diharapkan untuk mengetahui N-gram terbaik. Pada percobaan ini, digunakan jumlah cluster sebanyak 2. Percobaan dilakukan sebanyak 3 kali dengan mengubah parameter \"n\" pada N-gram. Maka didapat: n-gram Shilhouette Coeffisient 1 0.325 2 0.176 3 0.205 Dari hasil di atas, didapat nilai shilhouette tertinggi dengan n-gram 1, yaitu 0.325. Sementara paling rendah adalah 0.176, apabila nilai n-gram adalah 2. Selain itu, apabila dilakukan seleksi fitur, ngram > 1 akan menghasilkan 1 cluster, tidak peduli berapapun jumlah cluster yg ditentukan.","title":"Berdasarkan N-Gram"},{"location":"Evaluasi/#k-means","text":"Karena nilai Shilhouette yang relatif kecil, hasil dari Fuzzy C-Means akan dibandingkan dengan metode lain, yaitu K-Means, untuk melihat manakah yang lebih baik. Parameter yang digunakan di antaranya, Jumlah Cluster = 2, Threshold = 0.85, dan N-Gram = 1. Maka, diperoleh hasil berikut: Metode Shilhouette Coefisient Fuzzy C-Means 0.342 K-Means 0.430","title":"K-Means"},{"location":"Evaluasi/#kesimpulan","text":"Dari hasil percobaan di atas, dapat disimpulkan bahwa metode yang terbaik adalah K-Means, dengan parameter, n-gram=1, jumlah cluster=2, dan nilai Treshold=0.85.","title":"Kesimpulan"},{"location":"Evaluasi/#source-code-dan-output","text":"Source code lengkap dapat dilihat di sini . Source code hanya berjalan dengan parameter: N-gram => input user saat running Nilai Threshold untuk Seleksi Fitur: {0.9, 0.8} Cluster => 3 Cluster Untuk melakukan pengujian diatas, dilakukan pengubahan parameter secara manual berkali-kali. Salah satu hasil running bisa dilihat di repository dengan tipe file .csv . Berikut detail outputnya: test.db => berisi data yang telah dicrawl (format: SQLite ) bow_manual_1.csv => berisi hasil dari metode bag of word tfidf1.csv => berisi hasil dari metode tfidf seleksiFitur1.csv => berisi hasil dari metode Seleksi Fitur Cluster1.csv => berisi hasil dari Clustering tanpa dilakukan Seleksi Fitur Cluster1FS.csv => berisi hasil dari Clustering dengan dilakukan Seleksi Fitur","title":"Source Code dan Output"},{"location":"Graph/","text":"Graph \u00b6 Graph merupakan sebuah struktur data yang bersifat non linear yang berisi node dan edges. Node biasanya berupa titik, sementara edge berupa garis yang menghubungkan setiap node. Pada Web Structure Mining, Graph Berarah ( directed graph ) digunakan untuk menggambarkan hubungan suatu web. Beberapa terminology yang digunakan di Web Structure Mining adalah: Web Graph : Graph Berarah yang mewakili suatu web Node(s): Web Page pada Graph Edge(s): Hyperlink in degree : jumlah link yang menuju ke node tertentu out degree: jumlah link yang berasal dari node tertentu. Code \u00b6 Proses pembuatan Graph pada Python bisa dilakukan dengan mudah menggunakan library networkx dan mathplotlib untuk menampilkannya. import networkx as nx import matplotlib.pyplot as plt #membuat Graph g = nx . from_pandas_edgelist ( edgelistFrame , \"From\" , \"To\" , None , nx . DiGraph ()) # deklarasi pos (koordinat) (otomatis) pos = nx . spring_layout ( g ) # Membuat Label print ( \"keterangan node:\" ) nodelist = g . nodes label = {} for i , key in enumerate ( nodelist ): label [ key ] = i # Draw Graph nx . draw ( g , pos ) nx . draw_networkx_labels ( g , pos , label , font_color = \"b\" ) # show figure plt . axis ( \"off\" ) plt . show () Asumsi dari program di atas adalah kita sudah melakukan proses crawl di atas dan memiliki daftar edge yang berada di variabel edgelist . Pertama kita membuat sebuah Graph berarah dengan memanggil g = nx.Graph() lalu g = g.to_directed() . Lalu untuk memasukkan daftar edge yang telah dibuat ke graph, kita bisa menggunakan .from_pandas_edgelist dengan paraeter berturut-turut pandas.DataFrame, nama_kolom_sumber, nama_kolom_tujuan, edge_attribut, dan jenis_graph. Lalu kita mendefinisikan letak dari setiap node menggunakan layout yang tersedia di networkx, seperti nx.spring_layout(g) . Untuk menampilkan graph, kita menggunakan method nx.draw(graph, pos) . Kemudian plt.show() .","title":"Graph"},{"location":"Graph/#graph","text":"Graph merupakan sebuah struktur data yang bersifat non linear yang berisi node dan edges. Node biasanya berupa titik, sementara edge berupa garis yang menghubungkan setiap node. Pada Web Structure Mining, Graph Berarah ( directed graph ) digunakan untuk menggambarkan hubungan suatu web. Beberapa terminology yang digunakan di Web Structure Mining adalah: Web Graph : Graph Berarah yang mewakili suatu web Node(s): Web Page pada Graph Edge(s): Hyperlink in degree : jumlah link yang menuju ke node tertentu out degree: jumlah link yang berasal dari node tertentu.","title":"Graph"},{"location":"Graph/#code","text":"Proses pembuatan Graph pada Python bisa dilakukan dengan mudah menggunakan library networkx dan mathplotlib untuk menampilkannya. import networkx as nx import matplotlib.pyplot as plt #membuat Graph g = nx . from_pandas_edgelist ( edgelistFrame , \"From\" , \"To\" , None , nx . DiGraph ()) # deklarasi pos (koordinat) (otomatis) pos = nx . spring_layout ( g ) # Membuat Label print ( \"keterangan node:\" ) nodelist = g . nodes label = {} for i , key in enumerate ( nodelist ): label [ key ] = i # Draw Graph nx . draw ( g , pos ) nx . draw_networkx_labels ( g , pos , label , font_color = \"b\" ) # show figure plt . axis ( \"off\" ) plt . show () Asumsi dari program di atas adalah kita sudah melakukan proses crawl di atas dan memiliki daftar edge yang berada di variabel edgelist . Pertama kita membuat sebuah Graph berarah dengan memanggil g = nx.Graph() lalu g = g.to_directed() . Lalu untuk memasukkan daftar edge yang telah dibuat ke graph, kita bisa menggunakan .from_pandas_edgelist dengan paraeter berturut-turut pandas.DataFrame, nama_kolom_sumber, nama_kolom_tujuan, edge_attribut, dan jenis_graph. Lalu kita mendefinisikan letak dari setiap node menggunakan layout yang tersedia di networkx, seperti nx.spring_layout(g) . Untuk menampilkan graph, kita menggunakan method nx.draw(graph, pos) . Kemudian plt.show() .","title":"Code"},{"location":"Output Program/","text":"Output Program \u00b6 Source Code lengkap dapat dilihat di sini . Program melakukan proses Web Structure Mining dengan alamat awal https://garuda.ristekdikti.go.id . Ketika program dijalankan, maka program akan mendownload halaman web dan mencari semua link yang ada pada halaman tersebut. Semua link yang didapat akan dilakukan proses crawling kembali dan dicatat. Semua catatan mengenai url_sumber dan url_tujuan terdapat pada variabel edgelistFrame . Berikut merupakan beberapa 5 edge teratas: No Url Sumber Url Tujuan 1 http://garuda.ristekdikti.go.id/ http://sinta2.ristekdikti.go.id/ 2 http://sinta2.ristekdikti.go.id/ http://scopus.com/ 3 http://scopus.com/ http://scopus.com/ 4 http://scopus.com/ https://blog.scopus.com/ 5 http://scopus.com/ http://linkedin.com/ Setelah semua edge didapat, maka dibuatlah graph berarah dengan memasukkan semua edge yang terdapat di edgelistFrame . Maka akan tampil graph sebagai berikut: dengan detail node: ID Nama website 0 http://garuda.ristekdikti.go.id/ 1 http://sinta2.ristekdikti.go.id/ 2 http://scopus.com/ 3 https://blog.scopus.com/ 4 http://linkedin.com/ 5 https://twitter.com/ 6 http://facebook.com/ 7 http://youtube.com/ 8 http://elsevier.com/ 9 https://dev.elsevier.com/ 10 http://relx.com/ 11 http://simlitabmas.ristekdikti.go.id/ 12 http://ristekdikti.go.id/ 13 http://risbang.ristekdikti.go.id/ 14 http://forlap.ristekdikti.go.id/ 15 http://tkt.ristekdikti.go.id/ 16 http://arjuna.ristekdikti.go.id/ 17 http://sinta.ristekdikti.go.id/ 18 http://arjuna2.ristekdikti.go.id/ 19 http://forlap.dikti.go.id/ 20 https://forlap.ristekdikti.go.id/ 21 http://sigap.pddikti.ristekdikti.go.id/ 22 https://risbang.ristekdikti.go.id/ 23 https://arjuna.ristekdikti.go.id/ 24 https://frp.ristekdikti.go.id/ 25 http://rirn.ristekdikti.go.id/ 26 http://instagram.com/ 27 https://scholar.google.com/ 28 https://accounts.google.com/ Kemudian kita mencari pagerank masing-masing website. Berikut hasil dari pagerank, setelah diurutkan dari yang terbesar ke terkecil. No ID Nama Website PageRank 1 16 http://arjuna.ristekdikti.go.id/ 0.293866 2 28 https://accounts.google.com/ 0.041422 3 13 http://risbang.ristekdikti.go.id/ 0.031320 4 11 http://simlitabmas.ristekdikti.go.id/ 0.031320 5 21 http://sigap.pddikti.ristekdikti.go.id/ 0.030538 6 20 https://forlap.ristekdikti.go.id/ 0.030538 7 2 http://scopus.com/ 0.028233 8 0 http://garuda.ristekdikti.go.id/ 0.025761 9 27 https://scholar.google.com/ 0.025561 10 19 http://forlap.dikti.go.id/ 0.025561 11 15 http://tkt.ristekdikti.go.id/ 0.025414 12 7 http://youtube.com/ 0.024751 13 6 http://facebook.com/ 0.024751 14 5 https://twitter.com/ 0.024751 15 17 http://sinta.ristekdikti.go.id/ 0.022989 16 14 http://forlap.ristekdikti.go.id/ 0.022989 17 12 http://ristekdikti.go.id/ 0.022989 18 1 http://sinta2.ristekdikti.go.id/ 0.022789 19 18 http://arjuna2.ristekdikti.go.id/ 0.022427 20 10 http://relx.com/ 0.022326 21 9 https://dev.elsevier.com/ 0.022326 22 8 http://elsevier.com/ 0.022326 23 4 http://linkedin.com/ 0.022326 24 3 https://blog.scopus.com/ 0.022326 25 26 http://instagram.com/ 0.022080 26 25 http://rirn.ristekdikti.go.id/ 0.022080 27 24 https://frp.ristekdikti.go.id/ 0.022080 28 16 https://arjuna.ristekdikti.go.id/ 0.022080 29 13 https://risbang.ristekdikti.go.id/ 0.022080 Dari hasil atas, dapat diketahui bahwa website yang paling penting adalah http://arjuna.ristekdikti.go.id/ , dengan PageRank sebanyak 0.293866.","title":"Output Program"},{"location":"Output Program/#output-program","text":"Source Code lengkap dapat dilihat di sini . Program melakukan proses Web Structure Mining dengan alamat awal https://garuda.ristekdikti.go.id . Ketika program dijalankan, maka program akan mendownload halaman web dan mencari semua link yang ada pada halaman tersebut. Semua link yang didapat akan dilakukan proses crawling kembali dan dicatat. Semua catatan mengenai url_sumber dan url_tujuan terdapat pada variabel edgelistFrame . Berikut merupakan beberapa 5 edge teratas: No Url Sumber Url Tujuan 1 http://garuda.ristekdikti.go.id/ http://sinta2.ristekdikti.go.id/ 2 http://sinta2.ristekdikti.go.id/ http://scopus.com/ 3 http://scopus.com/ http://scopus.com/ 4 http://scopus.com/ https://blog.scopus.com/ 5 http://scopus.com/ http://linkedin.com/ Setelah semua edge didapat, maka dibuatlah graph berarah dengan memasukkan semua edge yang terdapat di edgelistFrame . Maka akan tampil graph sebagai berikut: dengan detail node: ID Nama website 0 http://garuda.ristekdikti.go.id/ 1 http://sinta2.ristekdikti.go.id/ 2 http://scopus.com/ 3 https://blog.scopus.com/ 4 http://linkedin.com/ 5 https://twitter.com/ 6 http://facebook.com/ 7 http://youtube.com/ 8 http://elsevier.com/ 9 https://dev.elsevier.com/ 10 http://relx.com/ 11 http://simlitabmas.ristekdikti.go.id/ 12 http://ristekdikti.go.id/ 13 http://risbang.ristekdikti.go.id/ 14 http://forlap.ristekdikti.go.id/ 15 http://tkt.ristekdikti.go.id/ 16 http://arjuna.ristekdikti.go.id/ 17 http://sinta.ristekdikti.go.id/ 18 http://arjuna2.ristekdikti.go.id/ 19 http://forlap.dikti.go.id/ 20 https://forlap.ristekdikti.go.id/ 21 http://sigap.pddikti.ristekdikti.go.id/ 22 https://risbang.ristekdikti.go.id/ 23 https://arjuna.ristekdikti.go.id/ 24 https://frp.ristekdikti.go.id/ 25 http://rirn.ristekdikti.go.id/ 26 http://instagram.com/ 27 https://scholar.google.com/ 28 https://accounts.google.com/ Kemudian kita mencari pagerank masing-masing website. Berikut hasil dari pagerank, setelah diurutkan dari yang terbesar ke terkecil. No ID Nama Website PageRank 1 16 http://arjuna.ristekdikti.go.id/ 0.293866 2 28 https://accounts.google.com/ 0.041422 3 13 http://risbang.ristekdikti.go.id/ 0.031320 4 11 http://simlitabmas.ristekdikti.go.id/ 0.031320 5 21 http://sigap.pddikti.ristekdikti.go.id/ 0.030538 6 20 https://forlap.ristekdikti.go.id/ 0.030538 7 2 http://scopus.com/ 0.028233 8 0 http://garuda.ristekdikti.go.id/ 0.025761 9 27 https://scholar.google.com/ 0.025561 10 19 http://forlap.dikti.go.id/ 0.025561 11 15 http://tkt.ristekdikti.go.id/ 0.025414 12 7 http://youtube.com/ 0.024751 13 6 http://facebook.com/ 0.024751 14 5 https://twitter.com/ 0.024751 15 17 http://sinta.ristekdikti.go.id/ 0.022989 16 14 http://forlap.ristekdikti.go.id/ 0.022989 17 12 http://ristekdikti.go.id/ 0.022989 18 1 http://sinta2.ristekdikti.go.id/ 0.022789 19 18 http://arjuna2.ristekdikti.go.id/ 0.022427 20 10 http://relx.com/ 0.022326 21 9 https://dev.elsevier.com/ 0.022326 22 8 http://elsevier.com/ 0.022326 23 4 http://linkedin.com/ 0.022326 24 3 https://blog.scopus.com/ 0.022326 25 26 http://instagram.com/ 0.022080 26 25 http://rirn.ristekdikti.go.id/ 0.022080 27 24 https://frp.ristekdikti.go.id/ 0.022080 28 16 https://arjuna.ristekdikti.go.id/ 0.022080 29 13 https://risbang.ristekdikti.go.id/ 0.022080 Dari hasil atas, dapat diketahui bahwa website yang paling penting adalah http://arjuna.ristekdikti.go.id/ , dengan PageRank sebanyak 0.293866.","title":"Output Program"},{"location":"Page Rank/","text":"Page Rank \u00b6 PageRank (PR) adalah sebuah algoritma yang digunakan oleh Mesin Pencarian Google untuk me-ranking website pada mesin pencari mereka. PageRank dinamakan berdasar penemunya, Larry Page, salah satu pendiri Google. PageRank bukan hanya algoritma yang digunakan oleh Google, tapi juga algoritma pertama yang digunakan oleh perusahaan dan paling terkenal. Algoritma \u00b6 Misalkan sebuah kumpulan kecil dari 4 web page: A , B , C , dan D . Link dari sebuah page ke dirininya sendiri, atau outbound link yang sama dari satu page ke page lain, diabaikan. Jika link yang menuju ke halaman A berasal dari B, C, dan D, maka PR(A) dapat dicari dengan rumus: $$ P R(A)=\\frac{P R(B)}{L(B)}+\\frac{P R(C)}{L(C)}+\\frac{P R(D)}{L(D)} $$ Dengan L(x) adalah jumlah link yang keluar dari halaman x dan PR(x) adalah nilai PageRank dari halaman (x). Misalkan saja halaman B memiliki tautan ke halaman C dan A, halaman C memiliki tautan ke halaman A, dan halaman D memiliki tautan ke ketiga halaman tersebut. Maka didapat: $$ P R(A)=\\frac{P R(B)}{2}+\\frac{P R(C)}{1}+\\frac{P R(D)}{3} $$ Selain itu, pada PageRank juga dipengaruhi oleh Damping Factor, maka rumus menjadi: $$ P R\\left(p_{i}\\right)=\\frac{1-d}{N}+d \\sum_{p_{j} \\in M\\left(p_{i}\\right)} \\frac{P R\\left(p_{j}\\right)}{L\\left(p_{j}\\right)} $$ dengan d adalah Damping Factor dan N adalah jumlah dokumen. Secara default, damping factor yang digunakan adalah 0.85. Code \u00b6 Kita bisa memanfaatkan library networkx untuk menghitung PageRank, dengan cara: import networkx as nx damping_factor = 0.85 max_iterr = 100 error_toleransi = 0.0001 pr = nx . pagerank ( g , alpha = damping_factor , max_iter = max_iterr , tol = error_toleransi ) Asumsi program sudah memiliki graph yang sudah dilakukan di langkah sebelumnya dan disimpan di variabel g. Keseluruhan program bisa dilihat di sini \u00b6","title":"PageRank"},{"location":"Page Rank/#page-rank","text":"PageRank (PR) adalah sebuah algoritma yang digunakan oleh Mesin Pencarian Google untuk me-ranking website pada mesin pencari mereka. PageRank dinamakan berdasar penemunya, Larry Page, salah satu pendiri Google. PageRank bukan hanya algoritma yang digunakan oleh Google, tapi juga algoritma pertama yang digunakan oleh perusahaan dan paling terkenal.","title":"Page Rank"},{"location":"Page Rank/#algoritma","text":"Misalkan sebuah kumpulan kecil dari 4 web page: A , B , C , dan D . Link dari sebuah page ke dirininya sendiri, atau outbound link yang sama dari satu page ke page lain, diabaikan. Jika link yang menuju ke halaman A berasal dari B, C, dan D, maka PR(A) dapat dicari dengan rumus: $$ P R(A)=\\frac{P R(B)}{L(B)}+\\frac{P R(C)}{L(C)}+\\frac{P R(D)}{L(D)} $$ Dengan L(x) adalah jumlah link yang keluar dari halaman x dan PR(x) adalah nilai PageRank dari halaman (x). Misalkan saja halaman B memiliki tautan ke halaman C dan A, halaman C memiliki tautan ke halaman A, dan halaman D memiliki tautan ke ketiga halaman tersebut. Maka didapat: $$ P R(A)=\\frac{P R(B)}{2}+\\frac{P R(C)}{1}+\\frac{P R(D)}{3} $$ Selain itu, pada PageRank juga dipengaruhi oleh Damping Factor, maka rumus menjadi: $$ P R\\left(p_{i}\\right)=\\frac{1-d}{N}+d \\sum_{p_{j} \\in M\\left(p_{i}\\right)} \\frac{P R\\left(p_{j}\\right)}{L\\left(p_{j}\\right)} $$ dengan d adalah Damping Factor dan N adalah jumlah dokumen. Secara default, damping factor yang digunakan adalah 0.85.","title":"Algoritma"},{"location":"Page Rank/#code","text":"Kita bisa memanfaatkan library networkx untuk menghitung PageRank, dengan cara: import networkx as nx damping_factor = 0.85 max_iterr = 100 error_toleransi = 0.0001 pr = nx . pagerank ( g , alpha = damping_factor , max_iter = max_iterr , tol = error_toleransi ) Asumsi program sudah memiliki graph yang sudah dilakukan di langkah sebelumnya dan disimpan di variabel g. Keseluruhan program bisa dilihat di sini","title":"Code"},{"location":"Web Structure Mining/","text":"Web Structure Mining \u00b6 Web Structure Mining merupakan salah satu implementasi dari Data Mining. Berbeda dengan Web Content Mining yang berfokus pada isi/konten dari suatu website, Web Structure Mining berfokus pada struktur link pada hypertext antar-dokumen. Secara sederhana, kegiatan Structure Mining merupakan proses mengambil pola hubungan suatu website dengan website lain. Media sosial seperti Facebook, misalnya. Pengguna facebook memiliki hubungan \"pertemanan\" dengan pengguna facebook lain. Jika digambarkan, pola hubungan tersebut akan membentuk sebuah Graph yang sangat besar. Umumnya, Web Structure Mining berfungsi untuk menemukan hubungan antara suatu webpage dengan webpage lain. Hubungan tersebut dapat menentukan apakah kedua webpage tersebut memiliki kemiripan, baik secara struktur maupun konten. Keduanya mungkin saja berada di satu web server yang dibuat oleh satu orang yang sama. Fungsi lainnya adalah untuk menemukan hirarki atau jaringan dari hyperlink pada website dari domain tertentu. Hal ini dapat membantu mengelompokkan alur informasi pada website yang mewakili domain tertentu, yang menghasilkan proses query jauh lebih cepat dan efisien. Code \u00b6 Sama seperti Content Mining, kita akan menggunakan Library requests dan BeautifulSoup4. Code utama untuk crawling adalah sebagai berikut: def getAllLinks ( src ): # Pencegahan eror apabila link yang diambil mati try : # Get page html page = requests . get ( src ) # Mengubah html ke object beautiful soup soup = BeautifulSoup ( page . content , 'html.parser' ) # GET all tag <a> tags = soup . findAll ( \"a\" ) links = [] for tag in tags : # Pencegahan eror apabila link tidak memiliki href try : # Get all link link = tag [ 'href' ] if not link in links and 'http' in link : links . append ( link ) except KeyError : pass return links except : #print(\"Error 404 : Page \"+src+\" not found\") return list () Code di atas hanya berfungsi untuk mendapatkan semua link pada sebuah website. Pertama, kita mendapatkan semua tag a dengan method .findAll(\"a\") . Lalu kita bisa menggunakan tag[\"href\"] untuk mendapatkan link. Lalu, untuk melakukan crawling secara rekursif, kita akan mendefinisikan sebuah function crawl , yaitu: def simplifiedURL ( url ): ''' asumsi: alamat url tidak mengandung http(s) (misalnya \"true-http-website.com\" atau www (misalnya \"true-www-website.com\") ''' # cek 1 : www if \"www.\" in url : ind = url . index ( \"www.\" ) + 4 url = \"http://\" + url [ ind :] # cek 3 : tanda / di akhir if url [ - 1 ] == \"/\" : url = url [: - 1 ] # Cek 4 : cuma domain utama parts = url . split ( \"/\" ) url = '' for i in range ( 3 ): url += parts [ i ] + \"/\" return url def crawl ( url , max_deep , show = False , deep = 0 , done = []): # returnnya ada di edgelist, global edgelist # menambah counter kedalaman deep += 1 # menyamakan format url, agar tidak ada url yg dobel url = simplifiedURL ( url ) #menampilkan proses if not url in done : # crawl semua link links = getAllLinks ( url ) done . append ( url ) if show : if deep == 1 : print ( url ) else : print ( \"|\" , end = \"\" ) for i in range ( deep - 1 ): print ( \"--\" , end = \"\" ) print ( url ) for link in links : # Membentuk format jalan (edge => (dari, ke)) link = simplifiedURL ( link ) edge = ( url , link ) # Mengecek jalan, apabila belum dicatat, maka dimasukkan ke list if not edge in edgelist : edgelist . append ( edge ) # Cek kedalaman, jika belum sampai terakhir, maka crawling. if ( deep != max_deep ): crawl ( link , max_deep , show , deep , done ) Method simplifiedURL berfungsi untuk menyeragamkan format url, menjadi http://urlweb.com/ Lalu, pada method crawl , dilakukan proses crawling secara rekursif. Parameter yang digunakan adalah: url : alamat web awal yang digunakan untuk crawling max_deep : kedalaman maksimal show : menampilkan proses/tidak deep : untuk pengecekan kedalaman. Jangan ubah parameter ini! Parameter ini digunakan untuk keperluan rekursif done : untuk pengecekan url yang telah dicrawl. Hal ini untuk mengecilkan waktu crawling. Jangan ubah parameter ini! Parameter ini digunakan untuk keperluan rekursif Proses pemanggilan method di atas bisa dilakukan dengan cara: import pandas as pd import requests from bs4 import BeautifulSoup # inisiasi variabel awal root = \"http://garuda.ristekdikti.go.id/\" nodelist = [ root ] edgelist = [] deep = 3 #crawl crawl ( root , deep , show = True ) edgelistFrame = pd . DataFrame ( edgelist , None , ( \"From\" , \"To\" ))","title":"Web Structure Mining"},{"location":"Web Structure Mining/#web-structure-mining","text":"Web Structure Mining merupakan salah satu implementasi dari Data Mining. Berbeda dengan Web Content Mining yang berfokus pada isi/konten dari suatu website, Web Structure Mining berfokus pada struktur link pada hypertext antar-dokumen. Secara sederhana, kegiatan Structure Mining merupakan proses mengambil pola hubungan suatu website dengan website lain. Media sosial seperti Facebook, misalnya. Pengguna facebook memiliki hubungan \"pertemanan\" dengan pengguna facebook lain. Jika digambarkan, pola hubungan tersebut akan membentuk sebuah Graph yang sangat besar. Umumnya, Web Structure Mining berfungsi untuk menemukan hubungan antara suatu webpage dengan webpage lain. Hubungan tersebut dapat menentukan apakah kedua webpage tersebut memiliki kemiripan, baik secara struktur maupun konten. Keduanya mungkin saja berada di satu web server yang dibuat oleh satu orang yang sama. Fungsi lainnya adalah untuk menemukan hirarki atau jaringan dari hyperlink pada website dari domain tertentu. Hal ini dapat membantu mengelompokkan alur informasi pada website yang mewakili domain tertentu, yang menghasilkan proses query jauh lebih cepat dan efisien.","title":"Web Structure Mining"},{"location":"Web Structure Mining/#code","text":"Sama seperti Content Mining, kita akan menggunakan Library requests dan BeautifulSoup4. Code utama untuk crawling adalah sebagai berikut: def getAllLinks ( src ): # Pencegahan eror apabila link yang diambil mati try : # Get page html page = requests . get ( src ) # Mengubah html ke object beautiful soup soup = BeautifulSoup ( page . content , 'html.parser' ) # GET all tag <a> tags = soup . findAll ( \"a\" ) links = [] for tag in tags : # Pencegahan eror apabila link tidak memiliki href try : # Get all link link = tag [ 'href' ] if not link in links and 'http' in link : links . append ( link ) except KeyError : pass return links except : #print(\"Error 404 : Page \"+src+\" not found\") return list () Code di atas hanya berfungsi untuk mendapatkan semua link pada sebuah website. Pertama, kita mendapatkan semua tag a dengan method .findAll(\"a\") . Lalu kita bisa menggunakan tag[\"href\"] untuk mendapatkan link. Lalu, untuk melakukan crawling secara rekursif, kita akan mendefinisikan sebuah function crawl , yaitu: def simplifiedURL ( url ): ''' asumsi: alamat url tidak mengandung http(s) (misalnya \"true-http-website.com\" atau www (misalnya \"true-www-website.com\") ''' # cek 1 : www if \"www.\" in url : ind = url . index ( \"www.\" ) + 4 url = \"http://\" + url [ ind :] # cek 3 : tanda / di akhir if url [ - 1 ] == \"/\" : url = url [: - 1 ] # Cek 4 : cuma domain utama parts = url . split ( \"/\" ) url = '' for i in range ( 3 ): url += parts [ i ] + \"/\" return url def crawl ( url , max_deep , show = False , deep = 0 , done = []): # returnnya ada di edgelist, global edgelist # menambah counter kedalaman deep += 1 # menyamakan format url, agar tidak ada url yg dobel url = simplifiedURL ( url ) #menampilkan proses if not url in done : # crawl semua link links = getAllLinks ( url ) done . append ( url ) if show : if deep == 1 : print ( url ) else : print ( \"|\" , end = \"\" ) for i in range ( deep - 1 ): print ( \"--\" , end = \"\" ) print ( url ) for link in links : # Membentuk format jalan (edge => (dari, ke)) link = simplifiedURL ( link ) edge = ( url , link ) # Mengecek jalan, apabila belum dicatat, maka dimasukkan ke list if not edge in edgelist : edgelist . append ( edge ) # Cek kedalaman, jika belum sampai terakhir, maka crawling. if ( deep != max_deep ): crawl ( link , max_deep , show , deep , done ) Method simplifiedURL berfungsi untuk menyeragamkan format url, menjadi http://urlweb.com/ Lalu, pada method crawl , dilakukan proses crawling secara rekursif. Parameter yang digunakan adalah: url : alamat web awal yang digunakan untuk crawling max_deep : kedalaman maksimal show : menampilkan proses/tidak deep : untuk pengecekan kedalaman. Jangan ubah parameter ini! Parameter ini digunakan untuk keperluan rekursif done : untuk pengecekan url yang telah dicrawl. Hal ini untuk mengecilkan waktu crawling. Jangan ubah parameter ini! Parameter ini digunakan untuk keperluan rekursif Proses pemanggilan method di atas bisa dilakukan dengan cara: import pandas as pd import requests from bs4 import BeautifulSoup # inisiasi variabel awal root = \"http://garuda.ristekdikti.go.id/\" nodelist = [ root ] edgelist = [] deep = 3 #crawl crawl ( root , deep , show = True ) edgelistFrame = pd . DataFrame ( edgelist , None , ( \"From\" , \"To\" ))","title":"Code"},{"location":"about/","text":"About Me \u00b6 Hello, my name is Ibnu Asro P. I'm on studying in Trunojoyo Univesity right now, It's one of university in Indonesia. I'm studying Information Engineering. I'm Familiar with a few programming language, such as Python, Java, and PHP. I'm interested in Machine Learning and some software Engineering. Got questions or something? Email me at arsip.ibara@gmail.com","title":"About Me"},{"location":"about/#about-me","text":"Hello, my name is Ibnu Asro P. I'm on studying in Trunojoyo Univesity right now, It's one of university in Indonesia. I'm studying Information Engineering. I'm Familiar with a few programming language, such as Python, Java, and PHP. I'm interested in Machine Learning and some software Engineering. Got questions or something? Email me at arsip.ibara@gmail.com","title":"About Me"},{"location":"crawling/","text":"Web Crawler adalah sebuah program yang melintasi struktur hypertext dari web, dimulai dari sebuah alamat awal (yang disebut seed) dan secara sekursif mengunjungi alamat web di dalam halaman web. Web Crawler juga dikenal sebagai web robot, spider, worm, walker dan wanderer. Semua search engine besar menggunakan crawler yang mampu melintasi internet secara terus-menerus, untuk menemukan dan mengambil halaman web sebanyak mungkin. Data-data tersebut bisa sangat bervariasi, seperti text, citra, audio, video dan lain sebagainya. Dalam program ini, kita akan melakukan crawling pada text, atau lebih dikenal dengan text Mining . 1 Pada bahasa pemrograman Python, terdapat dua cara untuk melakukan WEb Crawling, yaitu menggunakan Library BeautifulSoup4 dan Library Requests, dan cara kedua menggunakan Library Scapy. Pada Contoh ini, kita akan menggunakan BeautifulSoup4 dan Request . Berikut code untuk melakukan Crawling: from bs4 import BeautifulSoup import requests def crawl ( src ): global c page = requests . get ( src ) # Mengubah html ke object beautiful soup soup = BeautifulSoup ( page . content , 'html.parser' ) # Find all item items = soup . findAll ( class_ = 'article-item' ) #print ('Proses : %.2f' %((c/maxPage)*100) + '%'); c+=1 for item in items : judul = item . find ( class_ = 'title-article' ) . getText () authors = item . find ( class_ = \"author-article\" ) . findAll ( class_ = 'title-author' ) author = '' for i in authors : author = author + i . getText () + '; ' abstrack = item . find ( class_ = 'article-abstract' ) . find ( 'p' ) . getText () #pengecekan data redundant cursor = conn . execute ( 'select * from jurnal2 where judul=?' , ( judul ,)) cursor = cursor . fetchall () if ( len ( cursor ) == 0 ): conn . execute ( \"INSERT INTO jurnal2 \\ VALUES (?, ?, ?, ?)\" , ( judul , author , abstrack , kategori )); Perlu diingat bahwa, setiap web memiliki struktur html yang berbeda , maka jika kalian mengubah url web, maka perlu dilakukan penyesuaian pada code. Selanjutnya, mari kita bahas lebih detail lagi. Untuk mendapatkan tag html .yang diinginkan BeautifulSoup menyediakan 2 fungsi, yaitu 2 : soup.find(parameter) digunakan untuk mendapatkan satu tag html yang muncul pertama kali. Hasilnya berupa objek soup soup.findAll(parameter) digunakan untuk mendapatkan semua tag html tersebut. Hasilnya berupa list Sementara itu, untuk parameternya memiliki 3 macam. Kalian bisa mencari berdasarkan: tag html (seperti <p> , <div> , h1 dsb). contoh: code html <div><p>aku makan sayur</p></div> maka, untuk mendapatkan tag p adalah : soup.find(\"p\") class code html <div><p class='makan'>aku makan sayur</p></div> maka, untuk mendapatkan tag p adalah : soup.find(class_='makan') id code html <div><p id='sayur'>aku makan sayur</p></div> maka, untuk mendapatkan tag p adalah : soup.find(id='sayur') Lalu, untuk mendapatkan textnya, digunakan .getText() pada objek BeautifulSoup. Setelah kalian paham tentang bagaimana menggunakan library beautifulsoup, kita akan membahas bagaimana code di atas bekerja. items = soup . findAll ( class_ = 'article-item' ) Hal pertama yang kita lakukan adalah menentukan apa saja yang akan kita ambil. Pada kasus jurnal online ini, kita hanya akan mengambil judul, penulis, beserta abstraknya saja. Untuk itu, di sini kita mengambil semua paper, yang bisa kita lihat berada di class 'article-item'. for item in items : judul = item . find ( class_ = 'title-article' ) . getText () authors = item . find ( class_ = \"author-article\" ) . findAll ( class_ = 'title-author' ) author = '' for i in authors : author = author + i . getText () + '; ' abstrack = item . find ( class_ = 'article-abstract' ) . find ( 'p' ) . getText () Kemudian, untuk setiap paper, kita ambil judul (dengan class 'title-article'), penulis (dengan class 'author-article'), dan abstrak (dengan class 'article-abstract', lantas dicari tag p). #pengecekan data redundant cursor = conn . execute ( 'select * from jurnal2 where judul=?' , ( judul ,)) cursor = cursor . fetchall () if ( len ( cursor ) == 0 ): conn . execute ( \"INSERT INTO jurnal2 \\ VALUES (?, ?, ?, ?)\" , ( judul , author , abstrack , kategori )); Kemudian, memasukkan ke dalam database. Sebelum itu, perlu dilakukan pengecekan apakah ada data yang sama. Karena hal itu bisa mengganggu hasil akhir nanti. ADITYA, Bayu Rima. Penggunaan Web Crawler Untuk Menghimpun Tweets dengan Metode Pre-Processing Text Mining. JURNAL INFOTEL , [S.l.], v. 7, n. 2, p. 93-100, nov. 2015. ISSN 2460-0997. Available at: < http://ejournal.st3telkom.ac.id/index.php/infotel/article/view/35 >. Date accessed: 29 apr. 2019. doi: https://doi.org/10.20895/infotel.v7i2.35 . \u21a9 https://www.dataquest.io/blog/web-scraping-tutorial-python/ \u21a9","title":"Crawling"},{"location":"preprocessing/","text":"Tahap preprocessing merupakan tahap mengolah data agar data lebih mudah diproses. Ada banyak jenis dari preprocessing . Namun, di kesempatan kali ini kita hanya akan membahas tentang seleksi fitur. Seleksi Fitur \u00b6 Seleksi fitur merupakan salah satu cara untuk mengurangi dimensi fitur yang sangat banyak. Seperti pada kasus kita, Text Mining, jumlah fitur yang didapatkan bisa mencapai lebih dari 2000 kata yang berbeda. Namun, tidak semua kata tersebut benar-benar berpengaruh pada hasil akhir nantinya. Selain itu, kita tahu bahwa semakin banyak data yang diproses, maka lebih banyak biaya dan waktu yang digunakan untuk memprosesnya. Oleh karena itu, kita perlu melakukan pengurangan fitur tanpa mengurangi kualitas hasil akhir, misalnya dengan Seleksi Fitur. Pada dasarnya, seleksi fitur memiliki 3 tipe umum 1 : Wrapper Filter Embed Selain itu, Seleksi Fitur juga memiliki banyak sekali metode-metode, seperti Information Gain, Chi Square, Pearson, dll. Pearson Correlation \u00b6 Pendekatan Pearson merupakan pendekatan paling sederhana. Pada pendekatan ini, setiap fitur akan dihitung korelasinya. Berikut rumus untuk mencari korelasi Pearson 2 : \\varrho(X, C)=\\frac{E(X C)-E(X) E(C)}{\\sqrt{\\sigma^{2}(X) \\sigma^{2}(C)}}=\\frac{\\sum_{i}\\left(x_{i}-\\overline{x}_{i}\\right)\\left(c_{i}-\\overline{c}_{i}\\right)}{\\sqrt{\\sum_{i}\\left(x_{i}-\\overline{x}_{i}\\right)^{2} \\sum_{j}\\left(c_{j}-\\overline{c}_{j}\\right)^{2}}} \\varrho(X, C)=\\frac{E(X C)-E(X) E(C)}{\\sqrt{\\sigma^{2}(X) \\sigma^{2}(C)}}=\\frac{\\sum_{i}\\left(x_{i}-\\overline{x}_{i}\\right)\\left(c_{i}-\\overline{c}_{i}\\right)}{\\sqrt{\\sum_{i}\\left(x_{i}-\\overline{x}_{i}\\right)^{2} \\sum_{j}\\left(c_{j}-\\overline{c}_{j}\\right)^{2}}} Semakin tinggi nilainya, maka fitur tersebut semakin kuat korelasinya. Lalu fitur yang memiliki korelasi tinggi akan dibuang salah satunya. 3 Pendekatan ini digunakan untuk data tipe numerik . Codenya sebagai berikut: def pearsonCalculate ( data , u , v ): \"i, j is an index\" atas = 0 ; bawah_kiri = 0 ; bawah_kanan = 0 for k in range ( len ( data )): atas += ( data [ k , u ] - meanFitur [ u ]) * ( data [ k , v ] - meanFitur [ v ]) bawah_kiri += ( data [ k , u ] - meanFitur [ u ]) ** 2 bawah_kanan += ( data [ k , v ] - meanFitur [ v ]) ** 2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas / ( bawah_kiri * bawah_kanan ) def meanF ( data ): meanFitur = [] for i in range ( len ( data [ 0 ])): meanFitur . append ( sum ( data [:, i ]) / len ( data )) return np . array ( meanFitur ) def seleksiFiturPearson ( data , threshold ): global meanFitur meanFitur = meanF ( data ) u = 0 while u < len ( data [ 0 ]): dataBaru = data [:, : u + 1 ] meanBaru = meanFitur [: u + 1 ] v = u while v < len ( data [ 0 ]): if u != v : value = pearsonCalculate ( data , u , v ) if value < threshold : dataBaru = np . hstack (( dataBaru , data [:, v ] . reshape ( data . shape [ 0 ], 1 ))) meanBaru = np . hstack (( meanBaru , meanFitur [ v ])) v += 1 data = dataBaru meanFitur = meanBaru return data Untuk pemanggilan function di atas, kita hanya perlu memanggil function seleksiFiturPearson() dengan parameter data yang akan diseleksi ( type data harus numpy array ) dan batas nilai korelasi yang akan digunakan untuk membuang fitur yang mirip. Misalnya seperti ini: #Menentukan batas threshold data = numpy . array ( tfidf ) threshold = 0.8 fiturBaru = seleksiFiturPearson ( tfidf , threshold ) Hasil output hanya berupa fitur-fitur yang telah diseleksi. Li, Jundong, et al. \"Feature selection: A data perspective.\" ACM Computing Surveys (CSUR) 50.6 (2018): 94. \u21a9 Biesiada, Jacek, and Wlodzis\u0142aw Duch. \"Feature selection for high-dimensional data\u2014a Pearson redundancy based filter.\" Computer recognition systems 2 . Springer, Berlin, Heidelberg, 2007. 242-249. \u21a9 https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf \u21a9","title":"Preprocessing"},{"location":"preprocessing/#seleksi-fitur","text":"Seleksi fitur merupakan salah satu cara untuk mengurangi dimensi fitur yang sangat banyak. Seperti pada kasus kita, Text Mining, jumlah fitur yang didapatkan bisa mencapai lebih dari 2000 kata yang berbeda. Namun, tidak semua kata tersebut benar-benar berpengaruh pada hasil akhir nantinya. Selain itu, kita tahu bahwa semakin banyak data yang diproses, maka lebih banyak biaya dan waktu yang digunakan untuk memprosesnya. Oleh karena itu, kita perlu melakukan pengurangan fitur tanpa mengurangi kualitas hasil akhir, misalnya dengan Seleksi Fitur. Pada dasarnya, seleksi fitur memiliki 3 tipe umum 1 : Wrapper Filter Embed Selain itu, Seleksi Fitur juga memiliki banyak sekali metode-metode, seperti Information Gain, Chi Square, Pearson, dll.","title":"Seleksi Fitur"},{"location":"preprocessing/#pearson-correlation","text":"Pendekatan Pearson merupakan pendekatan paling sederhana. Pada pendekatan ini, setiap fitur akan dihitung korelasinya. Berikut rumus untuk mencari korelasi Pearson 2 : \\varrho(X, C)=\\frac{E(X C)-E(X) E(C)}{\\sqrt{\\sigma^{2}(X) \\sigma^{2}(C)}}=\\frac{\\sum_{i}\\left(x_{i}-\\overline{x}_{i}\\right)\\left(c_{i}-\\overline{c}_{i}\\right)}{\\sqrt{\\sum_{i}\\left(x_{i}-\\overline{x}_{i}\\right)^{2} \\sum_{j}\\left(c_{j}-\\overline{c}_{j}\\right)^{2}}} \\varrho(X, C)=\\frac{E(X C)-E(X) E(C)}{\\sqrt{\\sigma^{2}(X) \\sigma^{2}(C)}}=\\frac{\\sum_{i}\\left(x_{i}-\\overline{x}_{i}\\right)\\left(c_{i}-\\overline{c}_{i}\\right)}{\\sqrt{\\sum_{i}\\left(x_{i}-\\overline{x}_{i}\\right)^{2} \\sum_{j}\\left(c_{j}-\\overline{c}_{j}\\right)^{2}}} Semakin tinggi nilainya, maka fitur tersebut semakin kuat korelasinya. Lalu fitur yang memiliki korelasi tinggi akan dibuang salah satunya. 3 Pendekatan ini digunakan untuk data tipe numerik . Codenya sebagai berikut: def pearsonCalculate ( data , u , v ): \"i, j is an index\" atas = 0 ; bawah_kiri = 0 ; bawah_kanan = 0 for k in range ( len ( data )): atas += ( data [ k , u ] - meanFitur [ u ]) * ( data [ k , v ] - meanFitur [ v ]) bawah_kiri += ( data [ k , u ] - meanFitur [ u ]) ** 2 bawah_kanan += ( data [ k , v ] - meanFitur [ v ]) ** 2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas / ( bawah_kiri * bawah_kanan ) def meanF ( data ): meanFitur = [] for i in range ( len ( data [ 0 ])): meanFitur . append ( sum ( data [:, i ]) / len ( data )) return np . array ( meanFitur ) def seleksiFiturPearson ( data , threshold ): global meanFitur meanFitur = meanF ( data ) u = 0 while u < len ( data [ 0 ]): dataBaru = data [:, : u + 1 ] meanBaru = meanFitur [: u + 1 ] v = u while v < len ( data [ 0 ]): if u != v : value = pearsonCalculate ( data , u , v ) if value < threshold : dataBaru = np . hstack (( dataBaru , data [:, v ] . reshape ( data . shape [ 0 ], 1 ))) meanBaru = np . hstack (( meanBaru , meanFitur [ v ])) v += 1 data = dataBaru meanFitur = meanBaru return data Untuk pemanggilan function di atas, kita hanya perlu memanggil function seleksiFiturPearson() dengan parameter data yang akan diseleksi ( type data harus numpy array ) dan batas nilai korelasi yang akan digunakan untuk membuang fitur yang mirip. Misalnya seperti ini: #Menentukan batas threshold data = numpy . array ( tfidf ) threshold = 0.8 fiturBaru = seleksiFiturPearson ( tfidf , threshold ) Hasil output hanya berupa fitur-fitur yang telah diseleksi. Li, Jundong, et al. \"Feature selection: A data perspective.\" ACM Computing Surveys (CSUR) 50.6 (2018): 94. \u21a9 Biesiada, Jacek, and Wlodzis\u0142aw Duch. \"Feature selection for high-dimensional data\u2014a Pearson redundancy based filter.\" Computer recognition systems 2 . Springer, Berlin, Heidelberg, 2007. 242-249. \u21a9 https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf \u21a9","title":"Pearson Correlation"},{"location":"referensi2/","text":"Useful Link(s) \u00b6 Full Source Code Mengenai Apa itu Web Structure Mining dan Fungsinya Wikipedia : Web Mining Mengenai Apa itu Graph Contoh Sederhana Membuat Graph menggunakan NetworkX Mengenai PageRank dan implementasinya menggunakan NetworkX","title":"Referensi"},{"location":"referensi2/#useful-links","text":"Full Source Code Mengenai Apa itu Web Structure Mining dan Fungsinya Wikipedia : Web Mining Mengenai Apa itu Graph Contoh Sederhana Membuat Graph menggunakan NetworkX Mengenai PageRank dan implementasinya menggunakan NetworkX","title":"Useful Link(s)"},{"location":"textExtraction/","text":"Pada tahap ini, text yang diambil akan dilakukan ekstraksi teks. Terdapat beberapa tahapan, yaitu: Stopword Removal, yaitu menghilangkan kata-kata dan tanda baca yang yang tidak menjadi fokus search engine karena terlalu sering muncul seperti saya, kamu, dia, tatkala, dan lain-lain. 1 Stemming yaitu mengubah suatu kata menjadi kata dasar, seperti kata \"menggunakan\" menjadi \"guna\", \"memakan\" menajadi \"makan\". 2 Tokenisasi (n-gram) yaitu memecah kalimat per kata, seperti \"aku makan sayur\", menjadi \"aku\", \"makan\", \"sayur\". Dalam tokenisasi ini, terdapat variasi jumlah kata yang dipecah. Misal dipecah menjadi 2 suku kata, seperti \"aku makan sayur bayam\", menjadi \"aku makan\", \"makan sayur\", \"sayur bayam\". Hal tersebut lebih dikenal sebagai n-gram 3 Ketiga tahapan di atas sudah saya pecah menjadi dua method berbeda seperti pada code di bawah. Perlu diketahui, untuk memudahkan komputasi, code dibawah menggunakan library Sastrawi yang berfungsi untuk memproses text (seperti stopword dan stemming) khusus untuk bahasa indonesia. Jika text yang anda gunakan merupakan bahasa Inggris, maka bisa menggunakan Library nltk . from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory def preprosesing ( txt ): SWfactory = StopWordRemoverFactory () stopword = SWfactory . create_stop_word_remover () Sfactory = StemmerFactory () stemmer = Sfactory . create_stemmer () hasil = '' for i in txt . split (): if i . isalpha (): # Menghilangkan Kata tidak penting stop = stopword . remove ( i ) stem = stemmer . stem ( stop ) hasil += stem + ' ' return hasil def tokenisasi ( txt , ngram = 1 ): token = [] start = 0 end = ngram txtSplit = txt . split () while end <= len ( txtSplit ): tmp = txtSplit [ start : end ] frase = '' for i in tmp : frase += i + ' ' token . append ( frase ) end += 1 ; start += 1 ; return token Untuk penjelasan lebih detailnya sebagai berikut: Untuk melakukan Stopword pertama kita perlu menginisiasi StemmerFactory, dengan cara: from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory SWfactory = StopWordRemoverFactory () stopword = SWfactory . create_stop_word_remover () Kemudian tinggal lakukan stemming dengan cara: text = 'ini adalah sebuah kalimat lengkap' hasil = stopword . remove ( text ) Untuk melakukan Stemming pertama kita perlu menginisiasi StemmerFactory, dengan cara: from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory Sfactory = StemmerFactory () stemmer = Sfactory . create_stemmer () Kemudian tinggal lakukan stemming dengan cara: text = 'ini adalah sebuah kalimat lengkap' hasil = stemmer . stem ( text ) Namun, apabila dilakukan Stopword dan Stemming secara terpisah, masih ada kata yang tidak memiliki makna. Maka, digunakan ditambahkan sedikit pengecekan manual sebagai berikut: from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory def preprosesing ( txt ): SWfactory = StopWordRemoverFactory () stopword = SWfactory . create_stop_word_remover () Sfactory = StemmerFactory () stemmer = Sfactory . create_stemmer () hasil = '' for i in txt . split (): if i . isalpha (): # Menghilangkan Kata tidak penting stop = stopword . remove ( i ) stem = stemmer . stem ( stop ) hasil += stem + ' ' return hasil Lalu untuk melakukan tokenisasi, kita bisa menggunakan method String bawaan, yaitu .split() . Kemudian kita akan membuat kondisi untuk melakukan n-gram, seperti berikut: def tokenisasi ( txt , ngram = 1 ): token = [] start = 0 end = ngram txtSplit = txt . split () while end <= len ( txtSplit ): tmp = txtSplit [ start : end ] frase = '' for i in tmp : frase += i + ' ' token . append ( frase ) end += 1 ; start += 1 ; return token Setelah ketiganya dilakukan, maka akan kita bisa membuat sebuah Vector Space Model, dengan menggunakan metode Bag of Words dan TF-IDF. Metode Bag of Words \u00b6 Bag of Words merupakan salah satu metode untuk membuat sebuah Vector Space Model (VSM) dengan cara menghitung setiap kata pada setiap dokumen. 3 Contohnya seperti ini doc 1 : \"Topi saya bundar\" doc 2 : \"Bundar topi saya. Kalau tidak bundar, bukan topi saya\" Kita akan menghitung setiap kata tersebut. Maka didapat VSM sebagai berikut: No Doc Topi Saya Bundar Kalau Tidak Bukan 1 1 1 1 0 0 0 2 2 2 2 1 1 1 Code untuk melakukan perhitungan tersebut adalah sebagai berikut def countWord ( txt , ngram = 1 ): ''' Fungsi ini digunakan untuk menghitung setiap kata pada satu string ''' d = dict () token = tokenisasi ( txt , ngram ) for i in token : if d . get ( i ) == None : d [ i ] = txt . count ( i ) return d def add_row_VSM ( d ): ''' Fungsi ini digunakan untuk membangun VSM ''' #init baris baru VSM . append ([]) # memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya for i in VSM [ 0 ]: if d . get ( i ) == None : VSM [ - 1 ] . append ( 0 ) else : VSM [ - 1 ] . append ( d . pop ( i )); # memasukkan kata baru for i in d : VSM [ 0 ] . append ( i ) #fitur baru for j in range ( 1 , len ( VSM ) - 1 ): #VSM[j].insert(-2,0) VSM [ j ] . append ( 0 ) VSM [ - 1 ] . append ( d . get ( i )) Method counWord digunakan untuk menghitung banyaknya kata pada dokumen. Sementara method add_row_VSM digunakan untuk membuat sebuah matrix VSM tersebut. Proses pemanggilan fungsi-fungsi di atas dilakukan oleh code di bawah ini: cursor = conn . execute ( \"SELECT * from jurnal2\" ) cursor = cursor . fetchall () cursor = cursor [: 60 ] pertama = True corpus = list () label = list () c = 1 n = int ( input ( \"ngram : \" )) #n=1 for row in cursor : print ( 'Proses : %.2f ' % (( c / len ( cursor )) * 100 ) + '%' ); c += 1 label . append ( row [ - 1 ]) txt = row [ - 2 ] cleaned = preprosesing ( txt ) cleaned = cleaned [: - 1 ] corpus . append ( cleaned ) d = countWord ( cleaned , n ) if pertama : pertama = False VSM = list (( list (), list ())) for key in d : VSM [ 0 ] . append ( key ) VSM [ 1 ] . append ( d [ key ]) else : add_row_VSM ( d ) Lalu untuk menampilkan hasilnya (agar terlihat rapi) maka kita export ke csv. Serta kita pisahkan antara kata-kata dengan nilainya: write_csv ( \"bow_manual_ %d .csv\" % n , VSM ) feature_name = VSM [ 0 ] bow = np . array ( VSM [ 1 :]) TF-IDF \u00b6 Selain menggunakan Bag of Words, kita juga bisa menggunakan metode TF-IDF. Hal ini karena Bag of Word memiliki kelemahan tersendiri. 4 TF-IDF sendiri merupakan kepanjangan dari Term Frequence (frekuensi Kata) dan Invers Document Frequence (invers frekuensi Dokumen). Rumus TF-IDF sendiri terbilang mudah karena hanya TFxIDF. Kita telah mencari TF sebelumnya (yaitu Bag of Words), karena konsep keduanya yang memang sama. Sekarang kita tinggal mencari nilai IDF. Untuk mendapatkan IDF, pertama kita perlu mencari DF (frekuensi Dokumen). Misalnya: doc1 : Topi Saya Bundar, bundar topi saya doc2 : Matahari itu terlihat bundar Maka, bisa kita ketahui: Kata Jumlah Dokumen yang memiliki kata tersebut (DF) Topi 1 Saya 1 Bundar 2 Matahari 1 itu 1 terlihat 1 Setelah itu, kita lakukan invers pada setiap kata: $$ I D F(k a t a)=1+\\log \\left(\\frac{1}{1+D F(k a t a)}\\right) $$ Setelah ketemu, maka tinggal kita kalikan TFxIDF. Berikut code programnya: T F I D F=T F \\times I D F T F I D F=T F \\times I D F df = list () total_doc = bow . shape [ 0 ] for kolom in range ( len ( bow [ 0 ])): total = 0 for baris in range ( len ( bow )): if ( bow [ baris , kolom ] > 0 ): total += 1 df . append ( total ) df = np . array ( df ) idf = list () for i in df : tmp = 1 + log10 ( total_doc / ( 1 + i )) idf . append ( tmp ) idf = np . array ( idf ) tfidf = bow * idf https://rahmadya.com/2019/04/24/stopword-berbahasa-indonesia/ \u21a9 https://devtrik.com/python/steeming-bahasa-indonesia-python-sastrawi/ \u21a9 Sarkar, Dipanjan. Text analytics with Python: A practical real-world approach to gaining actionable insights from your data . Apress, 2016. \u21a9 \u21a9 https://prasetiautamacv.wordpress.com/2016/07/31/tf-idf-vsm-menggunakan-python/ \u21a9","title":"Text Extraction"},{"location":"textExtraction/#metode-bag-of-words","text":"Bag of Words merupakan salah satu metode untuk membuat sebuah Vector Space Model (VSM) dengan cara menghitung setiap kata pada setiap dokumen. 3 Contohnya seperti ini doc 1 : \"Topi saya bundar\" doc 2 : \"Bundar topi saya. Kalau tidak bundar, bukan topi saya\" Kita akan menghitung setiap kata tersebut. Maka didapat VSM sebagai berikut: No Doc Topi Saya Bundar Kalau Tidak Bukan 1 1 1 1 0 0 0 2 2 2 2 1 1 1 Code untuk melakukan perhitungan tersebut adalah sebagai berikut def countWord ( txt , ngram = 1 ): ''' Fungsi ini digunakan untuk menghitung setiap kata pada satu string ''' d = dict () token = tokenisasi ( txt , ngram ) for i in token : if d . get ( i ) == None : d [ i ] = txt . count ( i ) return d def add_row_VSM ( d ): ''' Fungsi ini digunakan untuk membangun VSM ''' #init baris baru VSM . append ([]) # memasukkan kata berdasarkan kata yang telah ditemukan sebelumnya for i in VSM [ 0 ]: if d . get ( i ) == None : VSM [ - 1 ] . append ( 0 ) else : VSM [ - 1 ] . append ( d . pop ( i )); # memasukkan kata baru for i in d : VSM [ 0 ] . append ( i ) #fitur baru for j in range ( 1 , len ( VSM ) - 1 ): #VSM[j].insert(-2,0) VSM [ j ] . append ( 0 ) VSM [ - 1 ] . append ( d . get ( i )) Method counWord digunakan untuk menghitung banyaknya kata pada dokumen. Sementara method add_row_VSM digunakan untuk membuat sebuah matrix VSM tersebut. Proses pemanggilan fungsi-fungsi di atas dilakukan oleh code di bawah ini: cursor = conn . execute ( \"SELECT * from jurnal2\" ) cursor = cursor . fetchall () cursor = cursor [: 60 ] pertama = True corpus = list () label = list () c = 1 n = int ( input ( \"ngram : \" )) #n=1 for row in cursor : print ( 'Proses : %.2f ' % (( c / len ( cursor )) * 100 ) + '%' ); c += 1 label . append ( row [ - 1 ]) txt = row [ - 2 ] cleaned = preprosesing ( txt ) cleaned = cleaned [: - 1 ] corpus . append ( cleaned ) d = countWord ( cleaned , n ) if pertama : pertama = False VSM = list (( list (), list ())) for key in d : VSM [ 0 ] . append ( key ) VSM [ 1 ] . append ( d [ key ]) else : add_row_VSM ( d ) Lalu untuk menampilkan hasilnya (agar terlihat rapi) maka kita export ke csv. Serta kita pisahkan antara kata-kata dengan nilainya: write_csv ( \"bow_manual_ %d .csv\" % n , VSM ) feature_name = VSM [ 0 ] bow = np . array ( VSM [ 1 :])","title":"Metode Bag of Words"},{"location":"textExtraction/#tf-idf","text":"Selain menggunakan Bag of Words, kita juga bisa menggunakan metode TF-IDF. Hal ini karena Bag of Word memiliki kelemahan tersendiri. 4 TF-IDF sendiri merupakan kepanjangan dari Term Frequence (frekuensi Kata) dan Invers Document Frequence (invers frekuensi Dokumen). Rumus TF-IDF sendiri terbilang mudah karena hanya TFxIDF. Kita telah mencari TF sebelumnya (yaitu Bag of Words), karena konsep keduanya yang memang sama. Sekarang kita tinggal mencari nilai IDF. Untuk mendapatkan IDF, pertama kita perlu mencari DF (frekuensi Dokumen). Misalnya: doc1 : Topi Saya Bundar, bundar topi saya doc2 : Matahari itu terlihat bundar Maka, bisa kita ketahui: Kata Jumlah Dokumen yang memiliki kata tersebut (DF) Topi 1 Saya 1 Bundar 2 Matahari 1 itu 1 terlihat 1 Setelah itu, kita lakukan invers pada setiap kata: $$ I D F(k a t a)=1+\\log \\left(\\frac{1}{1+D F(k a t a)}\\right) $$ Setelah ketemu, maka tinggal kita kalikan TFxIDF. Berikut code programnya: T F I D F=T F \\times I D F T F I D F=T F \\times I D F df = list () total_doc = bow . shape [ 0 ] for kolom in range ( len ( bow [ 0 ])): total = 0 for baris in range ( len ( bow )): if ( bow [ baris , kolom ] > 0 ): total += 1 df . append ( total ) df = np . array ( df ) idf = list () for i in df : tmp = 1 + log10 ( total_doc / ( 1 + i )) idf . append ( tmp ) idf = np . array ( idf ) tfidf = bow * idf https://rahmadya.com/2019/04/24/stopword-berbahasa-indonesia/ \u21a9 https://devtrik.com/python/steeming-bahasa-indonesia-python-sastrawi/ \u21a9 Sarkar, Dipanjan. Text analytics with Python: A practical real-world approach to gaining actionable insights from your data . Apress, 2016. \u21a9 \u21a9 https://prasetiautamacv.wordpress.com/2016/07/31/tf-idf-vsm-menggunakan-python/ \u21a9","title":"TF-IDF"}]}